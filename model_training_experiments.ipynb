{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shravan1994/Talking-data-User-demographic-prediction/blob/main/model_training_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bgWM7tydTMX"
      },
      "source": [
        "## Our Approach "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ60icKycnqm"
      },
      "source": [
        "<p>We did not get good accuracy with using single model to predict the user group</p>\n",
        "<p>\n",
        "In this notebook we tried another approach where we are trying to predict the gender first and using that predicted gender as a new features to predict the user demographic group.</p>\n",
        "\n",
        "<p>\n",
        "Also we trained seperate models for devices having events and the devices which don't have any events data.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZaoEFkqG_5M"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoL3FXmKJIdA",
        "outputId": "12decd97-93c0-461b-dbec-08916edfaa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 192.9 MB 86 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laXtWw5uBltn"
      },
      "source": [
        "## Loading the data into memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kcwp4f3coSX",
        "outputId": "26cc289c-9ced-425d-aed9-f5922703e986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cJh-eIv8hGO",
        "outputId": "88d7d0eb-db82-456b-b382-925b7d4ab20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "Downloading talkingdata-mobile-user-demographics.zip to /content\n",
            " 94% 267M/283M [00:02<00:00, 125MB/s]\n",
            "100% 283M/283M [00:02<00:00, 106MB/s]\n",
            "Archive:  /content/talkingdata-mobile-user-demographics.zip\n",
            "  inflating: app_events.csv.zip      \n",
            "  inflating: app_labels.csv.zip      \n",
            "  inflating: events.csv.zip          \n",
            "  inflating: gender_age_test.csv.zip  \n",
            "  inflating: gender_age_train.csv.zip  \n",
            "  inflating: label_categories.csv.zip  \n",
            "  inflating: phone_brand_device_model.csv.zip  \n",
            "  inflating: sample_submission.csv.zip  \n",
            "Archive:  /content/app_events.csv.zip\n",
            "  inflating: data/app_events.csv     \n",
            "Archive:  /content/app_labels.csv.zip\n",
            "  inflating: data/app_labels.csv     \n",
            "Archive:  /content/events.csv.zip\n",
            "  inflating: data/events.csv         \n",
            "Archive:  /content/gender_age_test.csv.zip\n",
            "  inflating: data/gender_age_test.csv  \n",
            "Archive:  /content/gender_age_train.csv.zip\n",
            "  inflating: data/gender_age_train.csv  \n",
            "Archive:  /content/label_categories.csv.zip\n",
            "  inflating: data/label_categories.csv  \n",
            "Archive:  /content/phone_brand_device_model.csv.zip\n",
            "  inflating: data/phone_brand_device_model.csv  \n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  import os\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = \".\"\n",
        "\n",
        "  !kaggle competitions download -c talkingdata-mobile-user-demographics\n",
        "\n",
        "  !unzip '/content/talkingdata-mobile-user-demographics.zip'\n",
        "  !unzip '/content/app_events.csv.zip' -d data\n",
        "  !unzip '/content/app_labels.csv.zip' -d data\n",
        "  !unzip '/content/events.csv.zip' -d data\n",
        "  !unzip '/content/gender_age_test.csv.zip' -d data\n",
        "  !unzip '/content/gender_age_train.csv.zip' -d data\n",
        "  !unzip '/content/label_categories.csv.zip' -d data\n",
        "  !unzip '/content/phone_brand_device_model.csv.zip' -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAFUXnEldOKm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from geopy.distance import geodesic, great_circle\n",
        "from scipy.sparse import hstack, save_npz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import log_loss, mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKhesGTOveYX"
      },
      "outputs": [],
      "source": [
        "# drive path to save models\n",
        "drive_path = '/content/drive/MyDrive/Colab Notebooks/Self case study/Talking Data User Demographics Prediction/models'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH2OAuMAdmUa"
      },
      "source": [
        "## Getting different features, using below methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pBrxgu6F8WD"
      },
      "source": [
        "### 1. Bag of categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-6iq79u_zsR"
      },
      "outputs": [],
      "source": [
        "def get_bag_of_categories(devices, all_events_df, all_app_events_df, app_labels_df, label_categories_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/categories_vectorizer.sav'\n",
        "  app_categories_df = app_labels_df.merge(label_categories_df, on='label_id', how='left')\n",
        "  app_categories_dict = dict(zip(app_categories_df.app_id, app_categories_df.category))\n",
        "  \n",
        "  device_list = devices['device_id']\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['event_id', 'device_id']]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  app_events = app_events.groupby('device_id').app_id.apply(lambda x: ' '.join([app_categories_dict.get(app_id) for app_id in x])).to_frame()\n",
        "  app_events['app_id'] = app_events['app_id'].str.lower()\n",
        "\n",
        "  app_events = devices.merge(app_events, on='device_id', how='left')\n",
        "  app_events['app_id'] = app_events['app_id'].fillna('na')\n",
        "\n",
        "  if training:\n",
        "    vocabulary = label_categories_df['category'].fillna('unknown').str.lower().unique().tolist() + ['na']\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    categories_bow_vector = vectorizer.fit_transform(app_events['app_id'])\n",
        "    \n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return categories_bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    categories_bow_vector = vectorizer.transform(app_events['app_id'])\n",
        "    return categories_bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xs6fhqVGBut"
      },
      "source": [
        "### 2. bag of installed app_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PvAAD8kBgaP"
      },
      "outputs": [],
      "source": [
        "def get_bag_of_apps(devices, app_labels_df, all_events_df, all_app_events_df, label_categories_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/apps_vectorizer.sav'\n",
        "\n",
        "  device_list = devices['device_id']\n",
        "  app_categories_df = app_labels_df.merge(label_categories_df, on='label_id', how='left')\n",
        "\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['event_id', 'device_id']]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  app_events = app_events.groupby('device_id').app_id.apply(lambda x: ' '.join([str(app_id) for app_id in x])).to_frame()\n",
        "  \n",
        "  app_events = devices.merge(app_events, on='device_id', how='left')\n",
        "  app_events['app_id'] = app_events['app_id'].fillna('na')\n",
        "\n",
        "  if training:\n",
        "    vocabulory = list(app_categories_df['app_id'].astype(str).unique()) + ['na']\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulory, binary=True)\n",
        "    apps_bow_vector = vectorizer.fit_transform(app_events['app_id'])\n",
        "\n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return apps_bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    categories_bow_vector = vectorizer.transform(app_events['app_id'])\n",
        "    return categories_bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c04U6h4RThV"
      },
      "source": [
        "### 3. Bag of active apps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX96SDXzRT97"
      },
      "outputs": [],
      "source": [
        "def get_bag_of_active_apps(devices, app_labels_df, all_events_df, all_app_events_df, label_categories_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/active_apps_vectorizer.sav'\n",
        "  print('bag of apps')\n",
        "  device_list = devices['device_id']\n",
        "  app_categories_df = app_labels_df.merge(label_categories_df, on='label_id', how='left')\n",
        "\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['event_id', 'device_id']]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  app_events = app_events[app_events['is_active'] == 1]\n",
        "  app_events = app_events.groupby('device_id').app_id.apply(lambda x: ' '.join([str(app_id) for app_id in x])).to_frame()\n",
        "  \n",
        "  app_events = devices.merge(app_events, on='device_id', how='left')\n",
        "  app_events['app_id'] = app_events['app_id'].fillna('na')\n",
        "\n",
        "  if training:\n",
        "    vocabulory = list(app_categories_df['app_id'].astype(str).unique()) + ['na']\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulory)\n",
        "    apps_bow_vector = vectorizer.fit_transform(app_events['app_id'])\n",
        "\n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return apps_bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    apps_bow_vector = vectorizer.transform(app_events['app_id'])\n",
        "    return apps_bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4AMIuGLKaTK"
      },
      "source": [
        "### 3. phone brand response encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4Bkb4UoKecs"
      },
      "outputs": [],
      "source": [
        "def get_phone_brand_features_response_enc(devices, phone_brand_df, y_train, label=None, training=False):\n",
        "  import pickle\n",
        "  import json\n",
        "\n",
        "  # if we want features for gender prediction \n",
        "  # then labels should be only 0, 1\n",
        "  # and otherwise, it would be 12 labels 0 to 11\n",
        "  if 'gender' in label:\n",
        "    labels = np.array([0, 1])\n",
        "  else:\n",
        "    labels = np.array(list(range(0,12)))\n",
        "\n",
        "  filename = f'{drive_path}/response_encoding_phone_brand{label}.pkl'\n",
        "\n",
        "  phone_brand_df = phone_brand_df.groupby('device_id').agg(\n",
        "      phone_brand = ('phone_brand', lambda x: ' '.join(np.unique(x)))\n",
        "  ).reset_index()\n",
        "\n",
        "  if training:\n",
        "    encoding_dict = {}\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model['label'] = y_train\n",
        "\n",
        "    for brand_name in brand_and_model['phone_brand'].unique():\n",
        "      if not brand_name:\n",
        "        continue\n",
        "\n",
        "      temp = brand_and_model[brand_and_model['phone_brand'] == brand_name]\n",
        "      totals = len(temp)\n",
        "      \n",
        "      encoding_list = []\n",
        "      for label in labels:\n",
        "        count = len(temp[temp['label'] == label])\n",
        "        encoding_list.append(count/totals)\n",
        "\n",
        "      encoding_dict.update({\n",
        "          brand_name: encoding_list\n",
        "      })\n",
        "      \n",
        "    brand_and_model[labels] = brand_and_model['phone_brand'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['phone_brand', 'label'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    # save to disk\n",
        "    json.dump(encoding_dict, open(filename, 'w'))\n",
        "\n",
        "    return brand_and_model\n",
        "  else:\n",
        "    encoding_dict = json.load(open(filename))\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model[labels] = brand_and_model['phone_brand'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['phone_brand'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    return brand_and_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68sLv36g_es"
      },
      "source": [
        "### 4. device model response encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_mwsXJqg_ti"
      },
      "outputs": [],
      "source": [
        "def get_device_model_features_response_enc(devices, phone_brand_df, y_train, label=None, training=False):\n",
        "  import pickle\n",
        "  import json\n",
        "\n",
        "  # if we want features for gender prediction \n",
        "  # then labels should be only 0, 1\n",
        "  # and otherwise, it would be 12 labels 0 to 11\n",
        "  if 'gender' in label:\n",
        "    labels = np.array([0, 1])\n",
        "  else:\n",
        "    labels = np.array(list(range(0,12)))\n",
        "\n",
        "  filename = f'{drive_path}/response_encoding_device_model_{label}.pkl'\n",
        "\n",
        "  phone_brand_df = phone_brand_df.groupby('device_id').agg(\n",
        "    device_model = ('device_model', lambda x: ' '.join(np.unique(x)))\n",
        "  ).reset_index()\n",
        "\n",
        "  if training:\n",
        "    encoding_dict = {}\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model['label'] = y_train\n",
        "\n",
        "    for device_model in brand_and_model['device_model'].unique():\n",
        "      if not device_model:\n",
        "        continue\n",
        "\n",
        "      temp = brand_and_model[brand_and_model['device_model'] == device_model]\n",
        "      totals = len(temp)\n",
        "      \n",
        "      encoding_list = []\n",
        "      for label in labels:\n",
        "        count = len(temp[temp['label'] == label])\n",
        "        encoding_list.append(count/totals)\n",
        "\n",
        "      encoding_dict.update({\n",
        "          device_model: encoding_list\n",
        "      })\n",
        "\n",
        "    brand_and_model[labels] = brand_and_model['device_model'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['device_model', 'label'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    # save to disk\n",
        "    json.dump(encoding_dict, open(filename, 'w'))\n",
        "    return brand_and_model\n",
        "  else:\n",
        "    encoding_dict = json.load(open(filename))\n",
        "\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model[labels] = brand_and_model['device_model'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['device_model'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    return brand_and_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFKnG_NFoazI"
      },
      "source": [
        "### 5. Bag of Phone brands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhQ9LEPQQwxi"
      },
      "outputs": [],
      "source": [
        "def get_phone_brand_features_bow(devices, phone_brand_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/bow_phone_brand.pkl'\n",
        "  vocabulary = phone_brand_df['phone_brand'].str.lower().unique().tolist()\n",
        "\n",
        "  brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "  brand_and_model = brand_and_model.groupby('device_id').phone_brand.apply(lambda x: ' '.join([s for s in x])).to_frame()\n",
        "  \n",
        "  if training:\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    bow_vector = vectorizer.fit_transform(brand_and_model['phone_brand'])\n",
        "    \n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    bow_vector = vectorizer.transform(brand_and_model['phone_brand'])\n",
        "    return bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. label encoding phone brand"
      ],
      "metadata": {
        "id": "ndGEEHPrp_od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_phone_brand_features_label_encoded(devices, phone_brand_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/label_encode_phone_brand.pkl'\n",
        "  vocabulary = phone_brand_df['phone_brand'].str.lower().unique().tolist()\n",
        "\n",
        "  brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "  brand_and_model = brand_and_model.groupby('device_id').phone_brand.apply(lambda x: [r for r in x][0]).to_frame()\n",
        "  \n",
        "  if training:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(phone_brand_df['phone_brand'])\n",
        "    label_encoded = le.transform(brand_and_model['phone_brand'])\n",
        "    \n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(le, open(filename, 'wb'))\n",
        "    label_encoded.shape = (label_encoded.shape[0], 1)\n",
        "    return label_encoded\n",
        "  else:\n",
        "    le = pickle.load(open(filename, 'rb'))\n",
        "    label_encoded = le.transform(brand_and_model['phone_brand'])\n",
        "    label_encoded.shape = (label_encoded.shape[0], 1)\n",
        "    return label_encoded"
      ],
      "metadata": {
        "id": "XnqhwVC7p_zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')\n",
        "# encoded = get_phone_brand_features_label_encoded(devices, phone_brand_df, training=False)\n",
        "# print(devices.shape)\n",
        "# encoded.shape"
      ],
      "metadata": {
        "id": "zGOkkurRrtse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8ynm7mwTQAG"
      },
      "source": [
        "### 6. Bag of device model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsCE8YKJTTu4"
      },
      "outputs": [],
      "source": [
        "def get_device_model_features_bow(devices, phone_brand_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/bow_device_model.pkl'\n",
        "  vocabulary = phone_brand_df['device_model'].str.lower().unique().tolist()\n",
        "\n",
        "  device_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "  device_model = device_model.groupby('device_id').device_model.apply(lambda x: ' '.join([s for s in x])).to_frame()\n",
        "\n",
        "  if training:\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    bow_vector = vectorizer.fit_transform(device_model['device_model'])\n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    bow_vector = vectorizer.transform(device_model['device_model'])\n",
        "    return bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Label encoded device model"
      ],
      "metadata": {
        "id": "YYV6wGaDq9RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device_model_features_label_encoded(devices, phone_brand_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/label_encode_device_model.pkl'\n",
        "  vocabulary = phone_brand_df['device_model'].str.lower().unique().tolist()\n",
        "\n",
        "  brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "  brand_and_model = brand_and_model.groupby('device_id').device_model.apply(lambda x: [r for r in x][0]).to_frame()\n",
        "  \n",
        "  if training:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(phone_brand_df['device_model'])\n",
        "    label_encoded = le.transform(brand_and_model['device_model'])\n",
        "    \n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(le, open(filename, 'wb'))\n",
        "    label_encoded.shape = (label_encoded.shape[0], 1)\n",
        "    return label_encoded\n",
        "  else:\n",
        "    le = pickle.load(open(filename, 'rb'))\n",
        "    label_encoded = le.transform(brand_and_model['device_model'])\n",
        "    label_encoded.shape = (label_encoded.shape[0], 1)\n",
        "    return label_encoded"
      ],
      "metadata": {
        "id": "--081ypoq9YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')\n",
        "# get_device_model_features_label_encoded(devices, phone_brand_df, training=False)"
      ],
      "metadata": {
        "id": "WVqkzdfisuXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtsymZKRUgaA"
      },
      "source": [
        "### 7. Total events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djl1U2q6Ugh_"
      },
      "outputs": [],
      "source": [
        "def get_total_events(devices, all_events_df, training=False):\n",
        "  events = all_events_df[all_events_df['device_id'].isin(devices['device_id'])]\n",
        "  events = devices.merge(events, on='device_id', how='left')\n",
        "  events = events.groupby('device_id').event_id.apply(lambda x: len(x)).to_frame()\n",
        "  return events['event_id'].values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RR6WtbKVSGJ"
      },
      "source": [
        "### 8. Total app events per device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm_dON01VSMI"
      },
      "outputs": [],
      "source": [
        "def get_total_app_events(devices, all_events_df, all_app_events_df):\n",
        "  print('app events')\n",
        "  events = all_events_df[all_events_df['device_id'].isin(devices['device_id'])]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  app_events = app_events.groupby('device_id').event_id.apply(lambda x: len(x)).to_frame()\n",
        "  app_events = devices.merge(app_events, on='device_id', how='left')\n",
        "  return app_events['event_id'].values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9XfJm6nvKio"
      },
      "source": [
        "### 8. Total apps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alK3sMsFvKqx"
      },
      "outputs": [],
      "source": [
        "def total_apps_installed(devices, all_events_df, all_app_events_df, training=False):\n",
        "  device_list = devices['device_id']\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['event_id', 'device_id']]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  total_apps = app_events.groupby('device_id').agg({'app_id': 'nunique'}).reset_index()\n",
        "  total_apps = devices.merge(total_apps, on='device_id', how='left')\n",
        "  total_apps['app_id'] = total_apps.app_id.fillna(0)\n",
        "  total_apps['app_id'] = total_apps['app_id'].replace(np.inf, 0).replace(-np.inf, 0).replace(np.Infinity, 0).replace(np.NINF, 0).replace(float('inf'), 0)\n",
        "  features = total_apps['app_id'].values\n",
        "  features.shape = (features.shape[0], 1)\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBdHdppDKbXK"
      },
      "outputs": [],
      "source": [
        "# arr = total_apps_installed(devices, all_events_df, all_app_events_df)\n",
        "# # np.isinf(arr).any()\n",
        "# # list(arr)\n",
        "# print(arr.shape)\n",
        "# hourly_events = get_hourly_events(devices, all_events_df, training=True)\n",
        "# hstack(arr, hourly_events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEYXOPIgwQPV"
      },
      "source": [
        "### 9. distance travelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fovBZihgx0C1"
      },
      "outputs": [],
      "source": [
        "def distance_traveled_per_day(devices, all_events_df, training=False):\n",
        "    import pickle\n",
        "    filename = f'{drive_path}/distance_travelled_columns.sav'\n",
        "    def get_distance_travelled(x):\n",
        "        x = x.sort_values('datetime')\n",
        "        locations = x.to_dict('records')\n",
        "        total_d = 0\n",
        "        \n",
        "        for i, _ in enumerate(locations):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            \n",
        "            # ignoring the invalid lat and long co-ordinates\n",
        "            if locations[i-1].get('latitude') == 0.0 or locations[i].get('latitude') == 0.0 or locations[i-1].get('latitude') == 1.0 or locations[i].get('latitude') == 1.0:\n",
        "                continue\n",
        "\n",
        "            distance = great_circle(\n",
        "                (locations[i-1].get('latitude'), locations[i-1].get('longitude')), \n",
        "                (locations[i].get('latitude'), locations[i].get('longitude'))\n",
        "            ).km\n",
        "            total_d += distance\n",
        "        \n",
        "        return total_d\n",
        "    \n",
        "    device_list = devices['device_id']\n",
        "    device_events = all_events_df[all_events_df['device_id'].isin(device_list)].copy()\n",
        "    device_events['datetime'] = pd.to_datetime(device_events['timestamp'])\n",
        "    device_events['day'] = device_events['datetime'].dt.day\n",
        "\n",
        "    if training:\n",
        "      columns = list(device_events['day'].unique())\n",
        "      \n",
        "      # saving the vectorizer to drive\n",
        "      pickle.dump(columns, open(filename, 'wb'))\n",
        "    else:\n",
        "      columns = pickle.load(open(filename, 'rb'))\n",
        "      \n",
        "    print(columns)\n",
        "    device_events = device_events.groupby(['device_id', 'day'])[['datetime', 'latitude', 'longitude']].apply(\n",
        "      lambda x: get_distance_travelled(x)\n",
        "    ).to_frame().reset_index().rename(columns={0: 'distance'})\n",
        "    \n",
        "    device_distance_travelled = {}\n",
        "    for row in device_events.itertuples():\n",
        "      device_id = row.device_id\n",
        "      distance = row.distance\n",
        "      day = int(row.day)      \n",
        "  \n",
        "      if not device_distance_travelled.get(device_id):\n",
        "        device_distance_travelled.update({\n",
        "            device_id: {\n",
        "                int(day): 0 for day in columns\n",
        "            }\n",
        "        })\n",
        "        \n",
        "      device_distance_travelled[device_id].update({\n",
        "            'device_id': device_id,\n",
        "            int(day): distance\n",
        "        })\n",
        "\n",
        "    device_distance = pd.DataFrame(device_distance_travelled.values())\n",
        "    device_distance = devices.merge(device_distance, on='device_id', how='left')\n",
        "    device_distance = device_distance.drop('device_id', axis=1)\n",
        "    device_distance = device_distance.fillna(0)\n",
        "    return device_distance.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ocmyse_JzCqC"
      },
      "outputs": [],
      "source": [
        "# devices = pd.read_csv('data/gender_age_train.csv')\n",
        "# distance_travelled = distance_traveled_per_day(devices, all_events_df, training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5_fvHi4mh5A"
      },
      "outputs": [],
      "source": [
        "# distance_travelled.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vRQG-6zvAnI"
      },
      "source": [
        "### 10. Has events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B7q9Zs2vAwX"
      },
      "outputs": [],
      "source": [
        "def has_events(devices, all_events_df):\n",
        "  import math\n",
        "\n",
        "  device_has_events = all_events_df.groupby('device_id').agg({'event_id': 'nunique'}).reset_index()\n",
        "  device_events = devices.merge(device_has_events, on='device_id', how='left')\n",
        "  device_events['has_events'] = pd.notna(device_events['event_id'])\n",
        "  device_events['has_events'] = device_events['has_events'].replace({True: 1})\n",
        "  device_events['has_events'] = device_events['has_events'].replace({False: 0})\n",
        "  return device_events['has_events'].values    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxyLkg9DV5_d"
      },
      "source": [
        "### 11. hourly events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1Ah0Mu3V9Xm"
      },
      "outputs": [],
      "source": [
        "def get_hourly_events(devices, all_events_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/hourly_events.pkl'\n",
        "\n",
        "  events = all_events_df[all_events_df['device_id'].isin(devices['device_id'])]\n",
        "  # events['hour'] = pd.to_datetime(events['timestamp']).dt.hour\n",
        "  events['hour_day'] = pd.to_datetime(events['timestamp']).apply(lambda x: f\"{x.day}_{x.hour}\")\n",
        "  events = events.groupby(['device_id']).hour_day.apply(lambda x: ' '.join([str(s) for s in x])).to_frame().reset_index()\n",
        "  events = devices.merge(events, on='device_id', how='left')\n",
        "  events.hour_day = events.hour_day.fillna('na')\n",
        "\n",
        "  if training:\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_vector = vectorizer.fit_transform(events['hour_day'])\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    bow_vector = vectorizer.transform(events['hour_day'])\n",
        "    return bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG9dtpagcius"
      },
      "source": [
        "### Get median hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tQJixUlciCu"
      },
      "outputs": [],
      "source": [
        "def get_min_max_hourly_events(devices, all_events_df):\n",
        "  events = all_events_df[all_events_df['device_id'].isin(devices['device_id'])]\n",
        "  events['hour'] = pd.to_datetime(events['timestamp']).dt.hour\n",
        "  events = events.groupby(['device_id']).agg({\n",
        "      'hour': ['median', 'mean', 'min', 'max']\n",
        "  }).reset_index()\n",
        "  events = devices.merge(events, on='device_id', how='left')\n",
        "  events = events.drop('device_id', axis=1)\n",
        "  events = events.fillna(0)\n",
        "  \n",
        "  return events.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp1GQGYHl0Vo"
      },
      "outputs": [],
      "source": [
        "# events = get_hourly_events(devices, all_events_df)\n",
        "# events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXjSvXwm7EF3"
      },
      "source": [
        "### 11. median latitude and longitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ0Q4tWA7EZP"
      },
      "outputs": [],
      "source": [
        "def get_median_lat_long(devices, all_events_df):\n",
        "  device_list = devices['device_id']\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['device_id', 'latitude', 'longitude']]\n",
        "  lat_long = events.groupby('device_id').agg({\n",
        "      'latitude': 'median',\n",
        "      'longitude': 'median',\n",
        "  }).reset_index()\n",
        "  lat_long = devices.merge(lat_long, on='device_id', how='left')\n",
        "  lat_long[['latitude', 'longitude']] = lat_long[['latitude', 'longitude']].fillna(0)\n",
        "  lat_long[['latitude', 'longitude']] = lat_long[['latitude', 'longitude']].replace(np.inf, 0).replace(-np.inf, 0)\n",
        "  return lat_long[['latitude', 'longitude']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7e3Dx036-It"
      },
      "source": [
        "### 12. average age per phone brand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRytvGF56-Ul"
      },
      "outputs": [],
      "source": [
        "def get_phone_brand_avg_age(devices, phone_brand_df, train_data_devices, training=False):\n",
        "  import pickle\n",
        "  import json\n",
        "  \n",
        "  filename_brand = f'{drive_path}/phone_brand_avg_age.pkl'\n",
        "  filename_model = f'{drive_path}/device_model_avg_age.pkl'\n",
        "\n",
        "  brand_df = phone_brand_df.groupby('device_id').agg(\n",
        "      phone_brand = ('phone_brand', lambda x: ' '.join(np.unique(x)))\n",
        "  ).reset_index()\n",
        "\n",
        "  device_df = phone_brand_df.groupby('device_id').agg(\n",
        "      device_model = ('device_model', lambda x: ' '.join(np.unique(x)))\n",
        "  ).reset_index()\n",
        "  \n",
        "  if training:\n",
        "    brand_age_df = brand_df.merge(train_data_devices[['device_id', 'age']], on='device_id', how='left')\n",
        "    brands_avg = brand_age_df.groupby('phone_brand').agg({\n",
        "        'age': 'mean',\n",
        "    }).reset_index()\n",
        "    brands_avg['age'] = brands_avg.age.fillna(0)\n",
        "    brand_averages = dict(zip(brands_avg.phone_brand, brands_avg.age))\n",
        "\n",
        "    # device_df = devices.merge(device_df, on='device_id', how='left')\n",
        "    device_age_df = device_df.merge(train_data_devices[['device_id', 'age']], on='device_id', how='left')\n",
        "    model_avg = device_age_df.groupby('device_model').agg({\n",
        "        'age': 'mean',\n",
        "    }).reset_index()\n",
        "    model_avg['age'] = model_avg.age.fillna(0)\n",
        "    model_averages = dict(zip(model_avg.device_model, model_avg.age))\n",
        "\n",
        "    res_df = pd.DataFrame()\n",
        "    brand_and_model = devices.merge(brand_df, on='device_id', how='left')\n",
        "    res_df['avg_brand'] = brand_and_model['phone_brand'].apply(lambda x: brand_averages.get(x, 0))\n",
        "\n",
        "    brand_and_model = devices.merge(device_df, on='device_id', how='left')\n",
        "    res_df['avg_model'] = brand_and_model['device_model'].apply(lambda x: model_averages.get(x, 0))\n",
        "\n",
        "    json.dump(brand_averages, open(filename_brand, 'w'))\n",
        "    json.dump(model_averages, open(filename_model, 'w'))\n",
        "\n",
        "    return res_df[['avg_brand', 'avg_model']].values\n",
        "  else:\n",
        "    brand_averages = json.load(open(filename_brand, 'r'))\n",
        "    model_averages = json.load(open(filename_model, 'r'))\n",
        "    \n",
        "    res_df = pd.DataFrame()\n",
        "    brand_and_model = devices.merge(brand_df, on='device_id', how='left')\n",
        "    res_df['avg_brand'] = brand_and_model['phone_brand'].apply(lambda x: brand_averages.get(x, 0))\n",
        "\n",
        "    brand_and_model = devices.merge(device_df, on='device_id', how='left')\n",
        "    res_df['avg_model'] = brand_and_model['device_model'].apply(lambda x: model_averages.get(x, 0))\n",
        "    return res_df[['avg_brand', 'avg_model']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvGP__WqtMTq"
      },
      "outputs": [],
      "source": [
        "# devices = pd.read_csv('data/gender_age_train.csv')\n",
        "# phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')\n",
        "# # train_data_devices = pd.read_csv('data/gender_age_train.csv')\n",
        "\n",
        "# get_phone_brand_avg_age(devices, phone_brand_df, devices, training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aclw051E-pVf"
      },
      "source": [
        "### 13.count(distinct(app_id))/count(*) as appid_proportion,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4DJMaI4-psq"
      },
      "outputs": [],
      "source": [
        "def get_app_id_proportion(devices, all_events_df, all_app_events_df, training=False):\n",
        "  print('proportion appp_id')\n",
        "  events = all_events_df[all_events_df['device_id'].isin(devices['device_id'])]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  \n",
        "  \n",
        "  usage_proportion = app_events.groupby('device_id').agg(\n",
        "      app_proportion = ('app_id', lambda x: (len(np.unique(x)) / len(x)) if len(x) else 0)\n",
        "  ).reset_index()\n",
        "\n",
        "  usage_proportion = devices.merge(usage_proportion, how='left', on='device_id')\n",
        "  usage_proportion['app_proportion'] = usage_proportion['app_proportion'].fillna(0)\n",
        "\n",
        "  return usage_proportion['app_proportion'].values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrbvKFoPBrrw"
      },
      "source": [
        "### Extracting features in training data and save intermediate models to use on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwBpoO2FIs5n"
      },
      "outputs": [],
      "source": [
        "# all_events_df = pd.read_csv('data/events.csv')\n",
        "# all_app_events_df = pd.read_csv('data/app_events.csv')\n",
        "# app_labels_df = pd.read_csv('data/app_labels.csv')\n",
        "# label_categories_df = pd.read_csv('data/label_categories.csv')\n",
        "# phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY6idenCyfux"
      },
      "source": [
        "## Get features for training data.\n",
        "<p>We will also save the counter vectorizers used for multiple features, which we can use while preparing features for test data</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPdGwlxZW8g"
      },
      "outputs": [],
      "source": [
        "def select_k_best(features, y_train, label, n=1000, training=False):\n",
        "  from sklearn.feature_selection import SelectKBest, chi2\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/select_k_best_{label}.pkl'\n",
        "\n",
        "  if training:\n",
        "    kbest = SelectKBest(chi2, k=n)\n",
        "    best_features = kbest.fit_transform(features, y_train)\n",
        "    pickle.dump(kbest, open(filename, 'wb'))\n",
        "    return best_features\n",
        "  else:\n",
        "    kbest = pickle.load(open(filename, 'rb'))\n",
        "    best_features = kbest.transform(features)\n",
        "    return best_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wnVYjEVb7-j"
      },
      "outputs": [],
      "source": [
        "def remove_features_with_no_variance(features, training=False, label='with_events'):\n",
        "  from sklearn.feature_selection import VarianceThreshold\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/no_variance_model_{label}.pkl'\n",
        "  \n",
        "  if training:\n",
        "    selector = VarianceThreshold(threshold=0.0)\n",
        "    best_features = selector.fit_transform(features)\n",
        "    pickle.dump(selector, open(filename, 'wb'))\n",
        "    return best_features\n",
        "  else:\n",
        "    selector = pickle.load(open(filename, 'rb'))\n",
        "    best_features = selector.transform(features)\n",
        "    return best_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPh1Pnimyf5o"
      },
      "outputs": [],
      "source": [
        "def get_features_with_events(train_data_devices, y_train, training=False, label='gender'):\n",
        "  all_events_df = pd.read_csv('data/events.csv')\n",
        "  all_app_events_df = pd.read_csv('data/app_events.csv')\n",
        "  app_labels_df = pd.read_csv('data/app_labels.csv')\n",
        "  label_categories_df = pd.read_csv('data/label_categories.csv')\n",
        "  phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')\n",
        "\n",
        "  categories_bow = get_bag_of_categories(\n",
        "    train_data_devices, all_events_df, all_app_events_df, \n",
        "    app_labels_df, label_categories_df, training=training\n",
        "  )\n",
        "  apps_bow = get_bag_of_apps(train_data_devices, app_labels_df, all_events_df, all_app_events_df, label_categories_df, training=training)\n",
        "  active_app_bow = get_bag_of_active_apps(train_data_devices, app_labels_df, all_events_df, all_app_events_df, label_categories_df, training=training)\n",
        "  total_app_events = get_total_app_events(train_data_devices, all_events_df, all_app_events_df)\n",
        "  app_id_proportion = get_app_id_proportion(train_data_devices, all_events_df, all_app_events_df)\n",
        "\n",
        "  phone_brand_resp_enc = get_phone_brand_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_with_events', training=training)\n",
        "  device_model_resp_enc = get_device_model_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_with_events', training=training)\n",
        "\n",
        "  bow_phone_brand = get_phone_brand_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  bow_device_model = get_device_model_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  total_events = get_total_events(train_data_devices, all_events_df, training=training)\n",
        "  total_apps = total_apps_installed(train_data_devices, all_events_df, all_app_events_df, training=training)\n",
        "  distance_travelled = distance_traveled_per_day(train_data_devices, all_events_df, training=training)\n",
        "  median_lat_long = get_median_lat_long(train_data_devices, all_events_df)\n",
        "  hourly_events = get_hourly_events(train_data_devices, all_events_df, training=training)\n",
        "  median_hour = get_min_max_hourly_events(train_data_devices, all_events_df)\n",
        "\n",
        "  gender_age_train = pd.read_csv('data/gender_age_train.csv')\n",
        "  avg_age = get_phone_brand_avg_age(train_data_devices, phone_brand_df, gender_age_train, training=training).astype(float)\n",
        "  label_encoded_ph_brand = get_device_model_features_label_encoded(train_data_devices, phone_brand_df, training=training)\n",
        "  label_encoded_device_model = get_phone_brand_features_label_encoded(train_data_devices, phone_brand_df, training=training)\n",
        "\n",
        "  apps_bow_best = select_k_best(apps_bow, y_train, label=label, n=2000, training=training)\n",
        "\n",
        "  print(categories_bow.shape)\n",
        "  print(apps_bow_best.shape)\n",
        "  print(bow_phone_brand.shape)\n",
        "  print(bow_device_model.shape)\n",
        "  print(total_events.shape)\n",
        "  print(total_apps.shape)\n",
        "  print(distance_travelled.shape)\n",
        "  print(median_lat_long.shape)\n",
        "  print(hourly_events.shape)\n",
        "  print(total_app_events.shape)\n",
        "  print(active_app_bow.shape)\n",
        "  print(app_id_proportion.shape)\n",
        "  print(avg_age.shape)\n",
        "  print(median_hour.shape)\n",
        "  print(label_encoded_ph_brand.shape)\n",
        "  print(label_encoded_device_model.shape)\n",
        "\n",
        "  X = hstack([categories_bow, apps_bow_best, bow_phone_brand, bow_device_model,\n",
        "            total_events, phone_brand_resp_enc, device_model_resp_enc,\n",
        "            total_apps, distance_travelled, avg_age, \n",
        "            total_app_events, median_lat_long, median_hour, label_encoded_ph_brand, label_encoded_device_model\n",
        "  ])\n",
        "  # , app_id_proportion, hourly_events, total_app_events, active_app_bow, ,   # median_lat_long, total_app_events\n",
        "  # bad perf: active_app_bow\n",
        "  print('data shape before removing no variance data: ', X.shape)\n",
        "  print('training: ', training)\n",
        "  X = remove_features_with_no_variance(X, training=training, label=f'{label}_with_events')\n",
        "  print('data shape', X.shape)\n",
        "\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMBPUAsnuSw9"
      },
      "outputs": [],
      "source": [
        "def get_features_with_no_events(train_data_devices, y_train, training=False, label='gender'):\n",
        "  phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')\n",
        "  bow_phone_brand = get_phone_brand_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  bow_device_model = get_device_model_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  phone_brand_resp_enc = get_phone_brand_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_no_events', training=training)\n",
        "  device_model_resp_enc = get_device_model_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_no_events', training=training)\n",
        "  \n",
        "  gender_age_train = pd.read_csv('data/gender_age_train.csv')\n",
        "  avg_age = get_phone_brand_avg_age(train_data_devices, phone_brand_df, gender_age_train, training=training)\n",
        "  \n",
        "  label_encoded_ph_brand = get_device_model_features_label_encoded(train_data_devices, phone_brand_df, training=training)\n",
        "  label_encoded_device_model = get_phone_brand_features_label_encoded(train_data_devices, phone_brand_df, training=training)\n",
        "  \n",
        "  print(phone_brand_resp_enc.shape)\n",
        "  print(device_model_resp_enc.shape)\n",
        "  print(avg_age.shape)\n",
        "  print(label_encoded_ph_brand.shape)\n",
        "  print(label_encoded_device_model.shape)\n",
        "\n",
        "  X = hstack([phone_brand_resp_enc, device_model_resp_enc, bow_phone_brand, bow_device_model,\n",
        "              label_encoded_ph_brand, label_encoded_device_model, avg_age])\n",
        "  \n",
        "  X = remove_features_with_no_variance(X, training=training, label=f'{label}_no_events')\n",
        "  print('data shape', X.shape)\n",
        "  \n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mHmCXtFLzzp"
      },
      "outputs": [],
      "source": [
        "def get_labels(y, training=False, label='group'):\n",
        "  import pickle\n",
        "\n",
        "  filename = f'{drive_path}/label_encoder_{label}.pkl'\n",
        "\n",
        "  if training:\n",
        "    enc = LabelEncoder()\n",
        "    y =  enc.fit_transform(y)\n",
        "    pickle.dump(enc, open(filename, 'wb'))\n",
        "    return y\n",
        "  else:\n",
        "    enc = pickle.load(open(filename, 'rb'))\n",
        "    y =  enc.transform(y)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY-sg3RIyh34"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7ePo4QPyiBR"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(test_y, predict_y, class_labels):\n",
        "    C = confusion_matrix(test_y, predict_y)\n",
        "    print(\"Number of misclassified points \",(len(test_y)-np.trace(C))/len(test_y)*100)\n",
        "    \n",
        "    A =(((C.T)/(C.sum(axis=1))).T)\n",
        "    \n",
        "    B =(C/C.sum(axis=0))\n",
        "    print(1)\n",
        "    labels = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "    cmap=sns.light_palette(\"green\")\n",
        "\n",
        "    # representing A in heatmap format\n",
        "    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(2)\n",
        "    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(3)\n",
        "    # representing B in heatmap format\n",
        "    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2e3b-b8AMbs"
      },
      "source": [
        "## Predicting gender for devices with events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGcYSRLdAMpN"
      },
      "outputs": [],
      "source": [
        "def train_gender_predictor_with_events(devices_events_gen, y_gen_label, training=False):\n",
        "  X_train = get_features_with_events(devices_events_gen, y_gen_label, training=training, label='gender')\n",
        "  if training:\n",
        "    print('train dataset\\'s shape')\n",
        "    print(X_train.shape)\n",
        "    print(y_gen_label.shape)\n",
        "\n",
        "    gpu_dict = {\n",
        "      'objective': 'binary:logistic',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"logloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {\n",
        "        'max_depth': [2, 3, 4], \n",
        "        'n_estimators': [200, 300, 400, 500],\n",
        "        'scale_pos_weight': [10, 66]\n",
        "        }\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_train, y_gen_label)\n",
        "\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_train, y_gen_label)\n",
        "          \n",
        "    predict_y_train_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_train = cali_cfl.predict(X_train)\n",
        "      \n",
        "    # saving the model to disk for later usage\n",
        "    import pickle\n",
        "    file_name = f\"{drive_path}/best_model_gender_with_events.pkl\"\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y_train, predict_y_train_proba\n",
        "  else:\n",
        "    print('testing')\n",
        "    import pickle\n",
        "    file_name = f\"{drive_path}/best_model_gender_with_events.pkl\"\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_test_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_test = cali_cfl.predict(X_train)\n",
        "    return predict_y_test, predict_y_test_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZY9XPcxapQX"
      },
      "source": [
        "## Predicting gender based on devices with no events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJE_8-9Hapf9"
      },
      "outputs": [],
      "source": [
        "def train_gender_predictor_with_no_events(devices_no_events_gen, y_gen_label, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_gender_with_no_events.pkl\"\n",
        "  X_train = get_features_with_no_events(devices_no_events_gen, y_gen_label, training=training, label='gender')\n",
        "  if training:\n",
        "    print('train dataset\\'s shape')\n",
        "    print(X_train.shape)\n",
        "    print(y_gen_label.shape)\n",
        "\n",
        "    gpu_dict = {\n",
        "      'objective': 'binary:logistic',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"logloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {\n",
        "        'max_depth': [2, 3, 4], \n",
        "        'n_estimators': [200, 300, 400, 500],\n",
        "        'scale_pos_weight': [10, 20, 33, 60]\n",
        "        }\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_train, y_gen_label)\n",
        "\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_train, y_gen_label)\n",
        "          \n",
        "    predict_y_train_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_train = cali_cfl.predict(X_train)\n",
        "      \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y_train, predict_y_train_proba\n",
        "  else:\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_test_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_test = cali_cfl.predict(X_train)\n",
        "    return predict_y_test, predict_y_test_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIxG_qxPeZtX"
      },
      "source": [
        "## Predicting age on devices with events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39hzrAd_eaAb"
      },
      "outputs": [],
      "source": [
        "def train_age_predictor_with_events(devices_events, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_age_predictor_with_events.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_events(devices_events, y_tr_events, training=training, label='age')\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'reg:squarederror',\n",
        "      'tree_method': 'gpu_hist'\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBRegressor(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_mean_squared_error',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "              \n",
        "    predict_y = x_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(x_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y\n",
        "  else:\n",
        "    # loading the model\n",
        "    x_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y = x_cfl.predict(X_events)\n",
        "    return predict_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS_vr-7tqdjb"
      },
      "outputs": [],
      "source": [
        "def train_age_predictor_with_events_1(devices_events, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "\n",
        "  file_name = f\"{drive_path}/best_model_age_predictor_with_events.pkl\"\n",
        "  normaliser_file = f\"{drive_path}/predict_age_normaliser_events_file.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_events(devices_events, y_tr_events, training=training, label='age')\n",
        "  \n",
        "  if training:\n",
        "    scaler = MaxAbsScaler()\n",
        "    X_events = scaler.fit_transform(X_events)\n",
        "\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_events, y_tr_events)            \n",
        "    predict_y = lr.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(lr, open(file_name, \"wb\"))\n",
        "    pickle.dump(scaler, open(normaliser_file, \"wb\"))\n",
        "    \n",
        "    return predict_y\n",
        "  else:\n",
        "    # loading the model\n",
        "    scaler = pickle.load(open(normaliser_file, \"rb\"))\n",
        "    lr = pickle.load(open(file_name, \"rb\"))\n",
        "    X_events = scaler.transform(X_events)\n",
        "    predict_y = lr.predict(X_events)\n",
        "    return predict_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdsi0sPekTwM"
      },
      "outputs": [],
      "source": [
        "def train_age_predictor_with_events_classif(devices_events, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_age_predictor_with_events.pkl\"\n",
        "  normaliser_file = f\"{drive_path}/predict_age_normaliser_no_events_file.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_events(devices_events, y_tr_events, training=training, label='age')\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_events, y_tr_events)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    xgb = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y = xgb.predict(X_events)\n",
        "    predict_y_proba = xgb.predict_proba(X_events)\n",
        "    return predict_y, predict_y_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A-RVAKzOPuj"
      },
      "source": [
        "## Predicting age based on devices with no data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh9ocwpy29Ij"
      },
      "outputs": [],
      "source": [
        "def train_age_predictor_without_events(devices_events, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_age_predictor_without_events.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_no_events(devices_events, y_tr_events, training=training, label='age')\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'reg:squarederror',\n",
        "      'tree_method': 'gpu_hist'\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBRegressor(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_mean_squared_error',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "              \n",
        "    predict_y = x_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(x_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y\n",
        "  else:\n",
        "    # loading the model\n",
        "    x_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y = x_cfl.predict(X_events)\n",
        "    return predict_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgSQ3h9cP3sW"
      },
      "outputs": [],
      "source": [
        "def train_age_predictor_without_events_1(devices_events, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "\n",
        "  file_name = f\"{drive_path}/best_model_age_predictor_without_events.pkl\"\n",
        "  normaliser_file = f\"{drive_path}/predict_age_normaliser_no_events_file.pkl\"\n",
        "    \n",
        "  X_no_events = get_features_with_no_events(devices_events, y_tr_events, training=training, label='age')\n",
        "  \n",
        "  if training:\n",
        "    scaler = MaxAbsScaler()\n",
        "    X_no_events = scaler.fit_transform(X_no_events)\n",
        "\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_no_events, y_tr_events)            \n",
        "    predict_y = lr.predict(X_no_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(lr, open(file_name, \"wb\"))\n",
        "    pickle.dump(scaler, open(normaliser_file, \"wb\"))\n",
        "    \n",
        "    return predict_y\n",
        "  else:\n",
        "    # loading the model\n",
        "    scaler = pickle.load(open(normaliser_file, \"rb\"))\n",
        "    lr = pickle.load(open(file_name, \"rb\"))\n",
        "    X_no_events = scaler.transform(X_no_events)\n",
        "    predict_y = lr.predict(X_no_events)\n",
        "    return predict_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNleW3KLv6ly"
      },
      "outputs": [],
      "source": [
        "def train_age_predictor_without_events_classif(devices_events, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "\n",
        "  file_name = f\"{drive_path}/best_model_age_predictor_without_events.pkl\"\n",
        "  normaliser_file = f\"{drive_path}/predict_age_normaliser_no_events_file.pkl\"\n",
        "    \n",
        "  X_no_events = get_features_with_no_events(devices_events, y_tr_events, training=training, label='age')\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_no_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_no_events, y_tr_events)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_no_events)\n",
        "    predict_y = cali_cfl.predict(X_no_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    xgb = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y = xgb.predict(X_no_events)\n",
        "    predict_y_proba = xgb.predict_proba(X_no_events)\n",
        "    return predict_y, predict_y_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcZDugB0wHOE"
      },
      "source": [
        "## Predicting user group based on predicted gender and age ,from other models\n",
        "<p>We used naive bays to predict final user group</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR5miYP5wHl2"
      },
      "outputs": [],
      "source": [
        "def train_user_group_events_nb(X_data, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_group_predictor_with_events_nb.pkl\"\n",
        "\n",
        "  if training:\n",
        "    parameters = {'alpha': [0.00001, 0.0005, 0.0001, 0.005, 0.001, 0.05, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 1000, 10000]}\n",
        "  \n",
        "    nb_clf = MultinomialNB()  \n",
        "    grid_search = GridSearchCV(nb_clf, parameters, cv=5, scoring='neg_log_loss', verbose=10, n_jobs=-1)\n",
        "    grid_search.fit(X_data, y_tr_events)\n",
        "\n",
        "    print('best params: ', grid_search.best_params_)\n",
        "\n",
        "    nb_model = grid_search.best_estimator_\n",
        "\n",
        "    sig_clf = CalibratedClassifierCV(nb_model, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_data, y_tr_events)\n",
        "\n",
        "    y_pred = sig_clf.predict(X_data)\n",
        "    y_pred_proba = sig_clf.predict_proba(X_data)\n",
        "\n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(sig_clf, open(file_name, \"wb\"))\n",
        "    return y_pred, y_pred_proba\n",
        "  else:\n",
        "    sig_clf = pickle.load(open(file_name, \"rb\"))\n",
        "    y_pred = sig_clf.predict(X_data)\n",
        "    y_pred_proba = sig_clf.predict_proba(X_data)\n",
        "    return y_pred, y_pred_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMeuMoYP4BZn"
      },
      "outputs": [],
      "source": [
        "def train_user_group_events_xgb_with_events(\n",
        "    devices_events, \n",
        "    predicted_age_gender, \n",
        "    y_tr_label, \n",
        "    training=False\n",
        "  ):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_group_predictor_with_events_xgb.pkl\"\n",
        "\n",
        "  X_events = get_features_with_events(devices_events, y_tr_label, training=training, label='group')\n",
        "  X_events = hstack([X_events, predicted_age_gender])\n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_label)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_events, y_tr_label)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    print('testing part')\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    return predict_y, predict_y_proba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_user_group_events_xgb_without_events(\n",
        "    devices_no_events, \n",
        "    predicted_age_gender_no_events,\n",
        "    y_tr_no_events_label, \n",
        "    training=False\n",
        "  ):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_group_predictor_without_events_xgb.pkl\"\n",
        "\n",
        "  X_events = get_features_with_no_events(devices_no_events, y_tr_no_events_label, training=training, label='group')\n",
        "  X_events = hstack([X_events, predicted_age_gender_no_events])\n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_no_events_label)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_events, y_tr_no_events_label)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    print('testing part')\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    return predict_y, predict_y_proba"
      ],
      "metadata": {
        "id": "5RqR-lLWdr4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV5rBrITIZiQ"
      },
      "source": [
        "## Predicting user groups on devices with events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV-QuntbIZv3"
      },
      "outputs": [],
      "source": [
        "def train_user_group_with_events(devices_events, gender_pred, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_group_with_events.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_events(devices_events, y_tr_events, training=training, label='group')\n",
        "  X_events = hstack([X_events, gender_pred])\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_events, y_tr_events)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    print('testing part')\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    return predict_y, predict_y_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkjEMigHcI9W"
      },
      "source": [
        "## Predicting user group on devices with no events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSBCw1XFcJEP"
      },
      "outputs": [],
      "source": [
        "def train_user_group_with_no_events(devices_events, gender_pred, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_group_with_no_events.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_no_events(devices_events, y_tr_events, training=training, label='group')\n",
        "  X_events = hstack([X_events, gender_pred])\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_events, y_tr_events)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    return predict_y, predict_y_proba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8DKLagS-6HN"
      },
      "source": [
        "## Method to train and cross validate both models on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUvQlhoj-6OU"
      },
      "outputs": [],
      "source": [
        "def train_models(devices, all_events_df):\n",
        "  y = devices['group'].values\n",
        "  is_events = has_events(devices, all_events_df)\n",
        "\n",
        "  print('Predicting on devices with  events')\n",
        "  devices_with_events = devices[is_events == 1]\n",
        "  y_with_events = y[is_events == 1]\n",
        "\n",
        "  tr_devices_events, te_devices_events, y_tr_events, y_te_events = train_test_split(\n",
        "    devices_with_events, y_with_events, test_size=0.1, stratify=y_with_events\n",
        "  )\n",
        "  y_tr_label = get_labels(y_tr_events, training=True, label='group')\n",
        "  y_te_label = get_labels(y_te_events, training=False, label='group')\n",
        "\n",
        "  # predicting gender with devices on events data\n",
        "  y_tr_events_gen = tr_devices_events['gender']\n",
        "  y_te_events_gen = te_devices_events['gender']\n",
        "\n",
        "  y_tr_gen_label = get_labels(y_tr_events_gen, training=True, label='gender')\n",
        "  y_te_gen_label = get_labels(y_te_events_gen, training=False, label='gender')\n",
        "\n",
        "  y_train_gen_pred, y_train_gen_pred_proba = train_gender_predictor_with_events(\n",
        "      tr_devices_events[['device_id']], y_tr_gen_label, training=True)\n",
        "  y_test_gen_pred, y_test_gen_pred_proba = train_gender_predictor_with_events(\n",
        "      te_devices_events[['device_id']], y_te_gen_label, training=False)\n",
        "\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_gen_label, y_train_gen_pred_proba))\n",
        "  print (\"The test log loss is:\", log_loss(y_te_gen_label, y_test_gen_pred_proba))\n",
        "\n",
        "  # predicting group on devices with events\n",
        "  y_train_pred, y_train_pred_proba = train_user_group_with_events(\n",
        "      tr_devices_events[['device_id']], y_train_gen_pred_proba, y_tr_label, training=True)\n",
        "  y_test_pred, y_test_pred_proba = train_user_group_with_events(\n",
        "      te_devices_events[['device_id']], y_test_gen_pred_proba, y_te_label, training=False)\n",
        "\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_label, y_train_pred_proba))\n",
        "  print (\"The test log loss is:\", log_loss(y_te_label, y_test_pred_proba))\n",
        "\n",
        "  print('Predicting based on device without events')\n",
        "  devices_no_events = devices[is_events == False]\n",
        "  y_without_events = y[is_events == False]\n",
        "\n",
        "  tr_devices_no_events, te_devices_no_events, y_tr_no_events, y_te_no_events = train_test_split(\n",
        "    devices_no_events, y_without_events, test_size=0.1, stratify=y_without_events\n",
        "  )\n",
        "  y_tr_no_events_label = get_labels(y_tr_no_events, training=True, label='group')\n",
        "  y_te_no_events_label = get_labels(y_te_no_events, training=False, label='group')\n",
        "\n",
        "  # predicting gender on devices without events\n",
        "  y_tr_no_events_gen_label = get_labels(tr_devices_no_events['gender'], training=True, label='gender')\n",
        "  y_te_no_events_gen_label = get_labels(te_devices_no_events['gender'], training=False, label='gender')\n",
        "\n",
        "  y_train_gen_pred_no_events, y_train_gen_pred_proba_no_events = train_gender_predictor_with_no_events(\n",
        "      tr_devices_no_events[['device_id']], y_tr_no_events_gen_label, training=True)\n",
        "  y_test_gen_pred_no_events, y_test_gen_pred_proba_no_events = train_gender_predictor_with_no_events(\n",
        "      te_devices_no_events[['device_id']], y_te_no_events_gen_label, training=False\n",
        "    )\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_no_events_gen_label, y_train_gen_pred_proba_no_events))\n",
        "  print (\"The test log loss is:\", log_loss(y_te_no_events_gen_label, y_test_gen_pred_proba_no_events))\n",
        "\n",
        "  # predicting group on devices without events\n",
        "  y_train_pred_no_events, y_train_pred_no_events_proba = train_user_group_with_no_events(\n",
        "      tr_devices_no_events[['device_id']], y_train_gen_pred_proba_no_events, y_tr_no_events_label, training=True)\n",
        "  y_test_pred_no_events, y_test_pred_no_events_proba = train_user_group_with_no_events(\n",
        "      te_devices_no_events[['device_id']], y_test_gen_pred_proba_no_events, y_te_no_events_label, training=False)\n",
        "\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_no_events_label, y_train_pred_no_events_proba))\n",
        "  print (\"The train log loss is:\", log_loss(y_te_no_events_label, y_test_pred_no_events_proba))\n",
        "\n",
        "  y_true = np.concatenate([y_te_label, y_te_no_events_label])\n",
        "  y_pred = np.concatenate([y_test_pred_proba, y_test_pred_no_events_proba])\n",
        "\n",
        "  print (\"The overall test log loss is:\", log_loss(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGYgn6ci1A8f"
      },
      "outputs": [],
      "source": [
        "# if __name__ == '__main__':\n",
        "#   devices = pd.read_csv('data/gender_age_train.csv')\n",
        "#   all_events_df = pd.read_csv('data/events.csv')\n",
        "#   train_models(devices, all_events_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZ3wdbQmmZc"
      },
      "source": [
        "## Using seperate regression model for predicting age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjIARHxpSVIc"
      },
      "outputs": [],
      "source": [
        "# devices = pd.read_csv('data/gender_age_train.csv')\n",
        "# all_events_df = pd.read_csv('data/events.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWpZXkekfmvH"
      },
      "outputs": [],
      "source": [
        "def get_age_groups(age_list, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/age_group_encoder.pkl\"\n",
        "\n",
        "  def get_group(age):\n",
        "    if age < 23:\n",
        "      return '-23'\n",
        "    elif age >= 23 and age <= 26:\n",
        "      return '23-26'\n",
        "    elif age >= 27 and age <= 28:\n",
        "      return '27-28'\n",
        "    elif age >= 29 and age <= 31:\n",
        "      return '29-31'\n",
        "    elif age >= 32 and age <= 38:\n",
        "      return '32-38'\n",
        "    elif age >= 39 and age <= 43:\n",
        "      return '39-43'\n",
        "    else:\n",
        "      return '43+'\n",
        "\n",
        "  age_groups = []\n",
        "  for age in age_list:\n",
        "    age_groups.append(get_group(age))\n",
        "\n",
        "  if training:\n",
        "    enc = LabelEncoder()\n",
        "    y =  enc.fit_transform(age_groups)\n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(enc, open(file_name, \"wb\"))\n",
        "    return y\n",
        "  else:\n",
        "    # saving the model to disk for later usage\n",
        "    enc = pickle.load(open(file_name, \"rb\"))\n",
        "    y =  enc.transform(age_groups)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Trying one model1 for gender model 2 for predicting age and model 3 for predicting user group based on model 1 and model 2 predictions</b>"
      ],
      "metadata": {
        "id": "2ribYQWGTI-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOWEa1mqmnVs",
        "outputId": "ef77959f-8c85-4383-ed90-4c17e0d3a486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting on devices with events\n",
            "bag of apps\n",
            "app events\n",
            "proportion appp_id\n",
            "[1, 30, 2, 3, 4, 5, 6, 7, 8]\n",
            "(20978, 827)\n",
            "(20978, 2000)\n",
            "(20978, 131)\n",
            "(20978, 1596)\n",
            "(20978, 1)\n",
            "(20978, 1)\n",
            "(20978, 9)\n",
            "(20978, 2)\n",
            "(20978, 170)\n",
            "(20978, 1)\n",
            "(20978, 113212)\n",
            "(20978, 1)\n",
            "(20978, 2)\n",
            "(20978, 4)\n",
            "(20978, 1)\n",
            "(20978, 1)\n",
            "data shape before removing no variance data:  (20978, 4580)\n",
            "training:  True\n",
            "data shape (20978, 2833)\n",
            "train dataset's shape\n",
            "(20978, 2833)\n",
            "(20978,)\n",
            "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n",
            "Best params  {'max_depth': 4, 'n_estimators': 500, 'scale_pos_weight': 10}\n",
            "bag of apps\n",
            "app events\n",
            "proportion appp_id\n",
            "[1, 30, 2, 3, 4, 5, 6, 7, 8]\n",
            "(2331, 827)\n",
            "(2331, 2000)\n",
            "(2331, 131)\n",
            "(2331, 1596)\n",
            "(2331, 1)\n",
            "(2331, 1)\n",
            "(2331, 9)\n",
            "(2331, 2)\n",
            "(2331, 170)\n",
            "(2331, 1)\n",
            "(2331, 113212)\n",
            "(2331, 1)\n",
            "(2331, 2)\n",
            "(2331, 4)\n",
            "(2331, 1)\n",
            "(2331, 1)\n",
            "data shape before removing no variance data:  (2331, 4580)\n",
            "training:  False\n",
            "data shape (2331, 2833)\n",
            "testing\n",
            "Gender: The train log loss is: 0.37036301588167103\n",
            "Gender: The train log loss is: 0.5545992043909465\n",
            "bag of apps\n",
            "app events\n",
            "proportion appp_id\n",
            "[1, 30, 2, 3, 4, 5, 6, 7, 8]\n",
            "(20978, 827)\n",
            "(20978, 2000)\n",
            "(20978, 131)\n",
            "(20978, 1596)\n",
            "(20978, 1)\n",
            "(20978, 1)\n",
            "(20978, 9)\n",
            "(20978, 2)\n",
            "(20978, 170)\n",
            "(20978, 1)\n",
            "(20978, 113212)\n",
            "(20978, 1)\n",
            "(20978, 2)\n",
            "(20978, 4)\n",
            "(20978, 1)\n",
            "(20978, 1)\n",
            "data shape before removing no variance data:  (20978, 4600)\n",
            "training:  True\n",
            "data shape (20978, 2843)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 200}\n",
            "bag of apps\n",
            "app events\n",
            "proportion appp_id\n",
            "[1, 30, 2, 3, 4, 5, 6, 7, 8]\n",
            "(2331, 827)\n",
            "(2331, 2000)\n",
            "(2331, 131)\n",
            "(2331, 1596)\n",
            "(2331, 1)\n",
            "(2331, 1)\n",
            "(2331, 9)\n",
            "(2331, 2)\n",
            "(2331, 170)\n",
            "(2331, 1)\n",
            "(2331, 113212)\n",
            "(2331, 1)\n",
            "(2331, 2)\n",
            "(2331, 4)\n",
            "(2331, 1)\n",
            "(2331, 1)\n",
            "data shape before removing no variance data:  (2331, 4600)\n",
            "training:  False\n",
            "data shape (2331, 2843)\n",
            "Age: The train logloss is: 1.4453870707985639\n",
            "Age: The train logloss is: 1.776000595856111\n",
            "(46202, 2)\n",
            "(46202, 2)\n",
            "(46202, 2)\n",
            "(46202, 1)\n",
            "(46202, 1)\n",
            "data shape (46202, 1054)\n",
            "train dataset's shape\n",
            "(46202, 1054)\n",
            "(46202,)\n",
            "Fitting 4 folds for each of 48 candidates, totalling 192 fits\n",
            "Best params  {'max_depth': 4, 'n_estimators': 500, 'scale_pos_weight': 10}\n",
            "(5134, 2)\n",
            "(5134, 2)\n",
            "(5134, 2)\n",
            "(5134, 1)\n",
            "(5134, 1)\n",
            "data shape (5134, 1054)\n",
            "The train log loss is: 0.6214302707561783\n",
            "The test log loss is: 0.655087913402395\n",
            "(46202, 12)\n",
            "(46202, 12)\n",
            "(46202, 2)\n",
            "(46202, 1)\n",
            "(46202, 1)\n",
            "data shape (46202, 1064)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 200}\n",
            "(5134, 12)\n",
            "(5134, 12)\n",
            "(5134, 2)\n",
            "(5134, 1)\n",
            "(5134, 1)\n",
            "data shape (5134, 1064)\n",
            "The train logloss is: 1.760282442529562\n",
            "The train logloss is: 1.8885886979553483\n",
            "train data\n",
            "(67180, 9)\n",
            "(67180,)\n",
            "test data\n",
            "(7465, 9)\n",
            "(7465,)\n",
            "bag of apps\n",
            "app events\n",
            "proportion appp_id\n",
            "[1, 30, 2, 3, 4, 5, 6, 7, 8]\n",
            "(20978, 827)\n",
            "(20978, 2000)\n",
            "(20978, 131)\n",
            "(20978, 1596)\n",
            "(20978, 1)\n",
            "(20978, 1)\n",
            "(20978, 9)\n",
            "(20978, 2)\n",
            "(20978, 170)\n",
            "(20978, 1)\n",
            "(20978, 113212)\n",
            "(20978, 1)\n",
            "(20978, 2)\n",
            "(20978, 4)\n",
            "(20978, 1)\n",
            "(20978, 1)\n",
            "data shape before removing no variance data:  (20978, 4600)\n",
            "training:  True\n",
            "data shape (20978, 2853)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 200}\n",
            "bag of apps\n",
            "app events\n",
            "proportion appp_id\n",
            "[1, 30, 2, 3, 4, 5, 6, 7, 8]\n",
            "(2331, 827)\n",
            "(2331, 2000)\n",
            "(2331, 131)\n",
            "(2331, 1596)\n",
            "(2331, 1)\n",
            "(2331, 1)\n",
            "(2331, 9)\n",
            "(2331, 2)\n",
            "(2331, 170)\n",
            "(2331, 1)\n",
            "(2331, 113212)\n",
            "(2331, 1)\n",
            "(2331, 2)\n",
            "(2331, 4)\n",
            "(2331, 1)\n",
            "(2331, 1)\n",
            "data shape before removing no variance data:  (2331, 4600)\n",
            "training:  False\n",
            "data shape (2331, 2853)\n",
            "testing part\n",
            "(46202, 12)\n",
            "(46202, 12)\n",
            "(46202, 2)\n",
            "(46202, 1)\n",
            "(46202, 1)\n",
            "data shape (46202, 1074)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 200}\n",
            "(5134, 12)\n",
            "(5134, 12)\n",
            "(5134, 2)\n",
            "(5134, 1)\n",
            "(5134, 1)\n",
            "data shape (5134, 1074)\n",
            "testing part\n",
            "The overall train log loss is: 1.8982973812468371\n",
            "The overall train log loss is: 2.388728255206195\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  devices = pd.read_csv('data/gender_age_train.csv')\n",
        "  all_events_df = pd.read_csv('data/events.csv')\n",
        "  \n",
        "  y = devices['group'].values\n",
        "\n",
        "  is_events = has_events(devices, all_events_df)\n",
        "\n",
        "  print('Predicting on devices with events')\n",
        "  devices_with_events = devices[is_events == 1]\n",
        "  y_with_events = y[is_events == 1]\n",
        "\n",
        "  tr_devices_events, te_devices_events, y_tr_events, y_te_events = train_test_split(\n",
        "    devices_with_events, y_with_events, test_size=0.1, stratify=y_with_events\n",
        "  )\n",
        "  y_tr_label = get_labels(y_tr_events, training=True, label='group')\n",
        "  y_te_label = get_labels(y_te_events, training=False, label='group')\n",
        "\n",
        "  # predicting gender with devices on events data\n",
        "  y_tr_events_gen = tr_devices_events['gender']\n",
        "  y_te_events_gen = te_devices_events['gender']\n",
        "\n",
        "  y_tr_gen_label = get_labels(y_tr_events_gen, training=True, label='gender')\n",
        "  y_te_gen_label = get_labels(y_te_events_gen, training=False, label='gender')\n",
        "\n",
        "  y_train_gen_pred, y_train_gen_pred_proba = train_gender_predictor_with_events(\n",
        "      tr_devices_events[['device_id']], y_tr_gen_label, training=True)\n",
        "  y_test_gen_pred, y_test_gen_pred_proba = train_gender_predictor_with_events(\n",
        "      te_devices_events[['device_id']], y_te_gen_label, training=False)\n",
        "\n",
        "  print (\"Gender: The train log loss is:\", log_loss(y_tr_gen_label, y_train_gen_pred_proba))\n",
        "  print (\"Gender: The train log loss is:\", log_loss(y_te_gen_label, y_test_gen_pred_proba))\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "  y_tr_events_age = get_age_groups(tr_devices_events['age'], training=True)\n",
        "  y_te_events_age = get_age_groups(te_devices_events['age'], training=False)\n",
        "\n",
        "  y_train_age_pred, y_train_age_pred_prob = train_age_predictor_with_events_classif(\n",
        "      tr_devices_events[['device_id']], y_tr_events_age, training=True)\n",
        "  y_test_age_pred, y_test_age_pred_prob  = train_age_predictor_with_events_classif(\n",
        "      te_devices_events[['device_id']], y_te_events_age, training=False)\n",
        "\n",
        "  print (\"Age: The train logloss is:\", log_loss(y_tr_events_age, y_train_age_pred_prob))\n",
        "  print (\"Age: The train logloss is:\", log_loss(y_te_events_age, y_test_age_pred_prob))\n",
        "\n",
        "  ####################################\n",
        "  # devices with no events\n",
        "  devices_no_events = devices[is_events == 0]\n",
        "  y_no_events = y[is_events == 0]\n",
        "\n",
        "  tr_devices_no_events, te_devices_no_events, y_tr_no_events, y_te_no_events = train_test_split(\n",
        "    devices_no_events, y_no_events, test_size=0.1, stratify=y_no_events\n",
        "  )\n",
        "\n",
        "  y_tr_no_events_label = get_labels(y_tr_no_events, training=True, label='group')\n",
        "  y_te_no_events_label = get_labels(y_te_no_events, training=False, label='group')\n",
        "\n",
        "  # predicting gender on devices without events\n",
        "  y_tr_no_events_gen_label = get_labels(tr_devices_no_events['gender'], training=True, label='gender')\n",
        "  y_te_no_events_gen_label = get_labels(te_devices_no_events['gender'], training=False, label='gender')\n",
        "\n",
        "  y_train_gen_pred_no_events, y_train_gen_pred_proba_no_events = train_gender_predictor_with_no_events(\n",
        "      tr_devices_no_events[['device_id']], y_tr_no_events_gen_label, training=True)\n",
        "  y_test_gen_pred_no_events, y_test_gen_pred_proba_no_events = train_gender_predictor_with_no_events(\n",
        "      te_devices_no_events[['device_id']], y_te_no_events_gen_label, training=False\n",
        "    )\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_no_events_gen_label, y_train_gen_pred_proba_no_events))\n",
        "  print (\"The test log loss is:\", log_loss(y_te_no_events_gen_label, y_test_gen_pred_proba_no_events))\n",
        "\n",
        "  # predicting age\n",
        "  y_tr_no_events_age = get_age_groups(tr_devices_no_events['age'], training=True)\n",
        "  y_te_no_events_age = get_age_groups(te_devices_no_events['age'], training=False)\n",
        "\n",
        "  y_train_no_event_age_pred, y_train_no_event_age_pred_proba = train_age_predictor_without_events_classif(tr_devices_no_events[['device_id']], y_tr_no_events_age, training=True)\n",
        "  y_test_no_event_age_pred, y_test_no_event_age_pred_proba  = train_age_predictor_without_events_classif(te_devices_no_events[['device_id']], y_te_no_events_age, training=False)\n",
        "\n",
        "  print (\"The train logloss is:\", log_loss(y_tr_no_events_age, y_train_no_event_age_pred_proba))\n",
        "  print (\"The train logloss is:\", log_loss(y_te_no_events_age, y_test_no_event_age_pred_proba))\n",
        "\n",
        "\n",
        "\n",
        "  # final model\n",
        "  X_events = np.hstack([y_train_gen_pred_proba, y_train_age_pred_prob])\n",
        "  X_no_events = np.hstack([y_train_gen_pred_proba_no_events, y_train_no_event_age_pred_proba])\n",
        "  x_train = np.vstack([X_events, X_no_events])\n",
        "  y_train_total = np.hstack([y_tr_label, y_tr_no_events_label])\n",
        "  print('train data')\n",
        "  print(x_train.shape)\n",
        "  print(y_train_total.shape)\n",
        "\n",
        "  X_events_test = np.hstack([y_test_gen_pred_proba, y_test_age_pred_prob])\n",
        "  X_no_events_test = np.hstack([y_test_gen_pred_proba_no_events, y_test_no_event_age_pred_proba])\n",
        "  x_test = np.vstack([X_events_test, X_no_events_test])\n",
        "  y_test_total = np.hstack([y_te_label, y_te_no_events_label])\n",
        "  print('test data')\n",
        "  print(x_test.shape)\n",
        "  print(y_test_total.shape)\n",
        "\n",
        "  y_train_events_pred, y_train_pred_events_proba = train_user_group_events_xgb_with_events(\n",
        "      tr_devices_events[['device_id']], X_events,\n",
        "      y_tr_label, training=True)\n",
        "\n",
        "  y_test_events_pred, y_test_pred_events_proba = train_user_group_events_xgb_with_events(\n",
        "      te_devices_events[['device_id']], X_events_test,\n",
        "      y_te_label, training=False)\n",
        "\n",
        "  y_train_no_events_pred, y_train_pred_no_events_proba = train_user_group_events_xgb_without_events(\n",
        "      tr_devices_no_events[['device_id']], X_no_events,\n",
        "      y_tr_no_events_label, training=True)\n",
        "\n",
        "  y_test_no_events_pred, y_test_pred_no_events_proba = train_user_group_events_xgb_without_events(\n",
        "      te_devices_no_events[['device_id']], X_no_events_test,\n",
        "      y_te_no_events_label, training=False)\n",
        "\n",
        "  y_train_total = np.concatenate([y_tr_label, y_tr_no_events_label])\n",
        "  y_train_pred_proba = np.concatenate([y_train_pred_events_proba, y_train_pred_no_events_proba])\n",
        "\n",
        "  y_test_total = np.concatenate([y_te_label, y_te_no_events_label])\n",
        "  y_test_pred_proba = np.concatenate([y_test_pred_events_proba, y_test_pred_no_events_proba])\n",
        "\n",
        "  print (\"The overall train log loss is:\", log_loss(y_train_total, y_train_pred_proba))\n",
        "  print (\"The overall train log loss is:\", log_loss(y_test_total, y_test_pred_proba))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.concatenate([y_tr_label, y_tr_no_events_label]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9UfuTuBBtZZ",
        "outputId": "0b1ddda8-feea-4d74-c201-8da4c758c0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67180,)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.concatenate([y_train_pred_events_proba, y_train_pred_no_events_proba]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB0NYe-40fS7",
        "outputId": "4e872293-44c2-49bd-b2cb-0237fc41415c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67180, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Gipo2kJFyy"
      },
      "outputs": [],
      "source": [
        "# df = pd.DataFrame({'a': [0,1,4, np.inf, -np.inf]})\n",
        "# # df = df.replace(np.inf, 0)\n",
        "# df.head()\n",
        "\n",
        "\n",
        "# print('inf', np.isfinite(df.values).all())\n",
        "\n",
        "# df.values  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe17iK9J09uP"
      },
      "outputs": [],
      "source": [
        "# if __name__ == '__main__':\n",
        "# devices = pd.read_csv('data/gender_age_train.csv')\n",
        "# all_events_df = pd.read_csv('data/events.csv')\n",
        "  # x_train, x_test = train_models_with_seperate_age_predictor(devices, all_events_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKCRbpeSvOL1"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8lZ5g4PvPzW"
      },
      "source": [
        "*   With same number of features and using the single model to predict user group, was giving very poor results.\n",
        "*   So, we, train four different models as described below along with their corresponding log loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTCj9zhjvyw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2af412-53dc-42d7-995d-58357db39072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is events data available?    What we are predicting?                                               Test log loss\n",
            "---------------------------  ------------------------------------------------------------------  ---------------\n",
            "Yes                          Gender                                                                     0.491647\n",
            "Yes                          User group (used predicted gender from above model as new feature)         2.1338\n",
            "No                           Gender                                                                     0.651126\n",
            "No                           User group (used predicted gender from above model as new feature)         2.42014\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  from tabulate import tabulate\n",
        "  data = [\n",
        "          ['Yes', \"Gender\", 0.49164734136922056],\n",
        "          [\"Yes\", \"User group (used predicted gender from above model as new feature)\", 2.133802776693025],\n",
        "          [\"No\", \"Gender\", 0.6511259420027772],\n",
        "          [\"No\", \"User group (used predicted gender from above model as new feature)\", 2.420144409974926]\n",
        "        ]\n",
        "  print(tabulate(data, headers=[\"Is events data available?\", \"What we are predicting?\", \"Test log loss\"]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_pBrxgu6F8WD",
        "9xs6fhqVGBut",
        "0c04U6h4RThV",
        "f4AMIuGLKaTK",
        "E68sLv36g_es",
        "YtsymZKRUgaA",
        "2RR6WtbKVSGJ",
        "E9XfJm6nvKio",
        "zEYXOPIgwQPV",
        "9vRQG-6zvAnI",
        "KxyLkg9DV5_d",
        "vG9dtpagcius",
        "eXjSvXwm7EF3",
        "Aclw051E-pVf",
        "nrbvKFoPBrrw",
        "QY-sg3RIyh34",
        "k2e3b-b8AMbs",
        "CIxG_qxPeZtX",
        "3A-RVAKzOPuj",
        "GV5rBrITIZiQ",
        "HkjEMigHcI9W"
      ],
      "name": "model_training_experiments.ipynb",
      "provenance": [],
      "mount_file_id": "1dWRE9GKISA4sBl2diRsa0t0gQlOOAuyn",
      "authorship_tag": "ABX9TyM8D3hqVL/BpBDXEDKlWN7/",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}