{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shravan1994/Talking-data-User-demographic-prediction/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Approach "
      ],
      "metadata": {
        "id": "0bgWM7tydTMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>We did not get good accuracy with using single model to predict the user group</p>\n",
        "<p>\n",
        "In this notebook we tried another approach where we are trying to predict the gender first and using that predicted gender as a new features to predict the user demographic group.</p>\n",
        "\n",
        "<p>\n",
        "Also we trained seperate models for devices having events and the devices which don't have any events data.\n",
        "</p>"
      ],
      "metadata": {
        "id": "uJ60icKycnqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-ZaoEFkqG_5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoL3FXmKJIdA",
        "outputId": "7ca84e43-44b1-439e-edc5-f7fda88d60e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laXtWw5uBltn"
      },
      "source": [
        "## Loading the data into memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kcwp4f3coSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0827e3f-4d30-4790-f8ed-18cbcba7a02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cJh-eIv8hGO",
        "outputId": "9cdadc22-8c5a-4f96-f3bb-b6b90a379c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "Downloading talkingdata-mobile-user-demographics.zip to /content\n",
            " 99% 281M/283M [00:05<00:00, 58.6MB/s]\n",
            "100% 283M/283M [00:05<00:00, 56.9MB/s]\n",
            "Archive:  /content/talkingdata-mobile-user-demographics.zip\n",
            "  inflating: app_events.csv.zip      \n",
            "  inflating: app_labels.csv.zip      \n",
            "  inflating: events.csv.zip          \n",
            "  inflating: gender_age_test.csv.zip  \n",
            "  inflating: gender_age_train.csv.zip  \n",
            "  inflating: label_categories.csv.zip  \n",
            "  inflating: phone_brand_device_model.csv.zip  \n",
            "  inflating: sample_submission.csv.zip  \n",
            "Archive:  /content/app_events.csv.zip\n",
            "  inflating: data/app_events.csv     \n",
            "Archive:  /content/app_labels.csv.zip\n",
            "  inflating: data/app_labels.csv     \n",
            "Archive:  /content/events.csv.zip\n",
            "  inflating: data/events.csv         \n",
            "Archive:  /content/gender_age_test.csv.zip\n",
            "  inflating: data/gender_age_test.csv  \n",
            "Archive:  /content/gender_age_train.csv.zip\n",
            "  inflating: data/gender_age_train.csv  \n",
            "Archive:  /content/label_categories.csv.zip\n",
            "  inflating: data/label_categories.csv  \n",
            "Archive:  /content/phone_brand_device_model.csv.zip\n",
            "  inflating: data/phone_brand_device_model.csv  \n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  import os\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = \".\"\n",
        "\n",
        "  !kaggle competitions download -c talkingdata-mobile-user-demographics\n",
        "\n",
        "  !unzip '/content/talkingdata-mobile-user-demographics.zip'\n",
        "  !unzip '/content/app_events.csv.zip' -d data\n",
        "  !unzip '/content/app_labels.csv.zip' -d data\n",
        "  !unzip '/content/events.csv.zip' -d data\n",
        "  !unzip '/content/gender_age_test.csv.zip' -d data\n",
        "  !unzip '/content/gender_age_train.csv.zip' -d data\n",
        "  !unzip '/content/label_categories.csv.zip' -d data\n",
        "  !unzip '/content/phone_brand_device_model.csv.zip' -d data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from geopy.distance import geodesic, great_circle\n",
        "from scipy.sparse import hstack, save_npz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "lAFUXnEldOKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKhesGTOveYX"
      },
      "outputs": [],
      "source": [
        "# drive path to save models\n",
        "drive_path = '/content/drive/MyDrive/Colab Notebooks/Self case study/Talking Data User Demographics Prediction/models'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting different features, using below methods"
      ],
      "metadata": {
        "id": "aH2OAuMAdmUa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pBrxgu6F8WD"
      },
      "source": [
        "### 1. Bag of categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-6iq79u_zsR"
      },
      "outputs": [],
      "source": [
        "def get_bag_of_categories(devices, all_events_df, all_app_events_df, app_labels_df, label_categories_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/categories_vectorizer.sav'\n",
        "  app_categories_df = app_labels_df.merge(label_categories_df, on='label_id', how='left')\n",
        "  app_categories_dict = dict(zip(app_categories_df.app_id, app_categories_df.category))\n",
        "  \n",
        "  device_list = devices['device_id']\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['event_id', 'device_id']]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  app_events = app_events.groupby('device_id').app_id.apply(lambda x: ' '.join([app_categories_dict.get(app_id) for app_id in x])).to_frame()\n",
        "  app_events['app_id'] = app_events['app_id'].str.lower()\n",
        "\n",
        "  app_events = devices.merge(app_events, on='device_id', how='left')\n",
        "  app_events['app_id'] = app_events['app_id'].fillna('na')\n",
        "\n",
        "  if training:\n",
        "    vocabulary = label_categories_df['category'].fillna('unknown').str.lower().unique().tolist() + ['na']\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    categories_bow_vector = vectorizer.fit_transform(app_events['app_id'])\n",
        "    \n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return categories_bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    categories_bow_vector = vectorizer.transform(app_events['app_id'])\n",
        "    return categories_bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xs6fhqVGBut"
      },
      "source": [
        "### 2. bag of installed app_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PvAAD8kBgaP"
      },
      "outputs": [],
      "source": [
        "def get_bag_of_apps(devices, app_labels_df, all_events_df, all_app_events_df, label_categories_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/apps_vectorizer.sav'\n",
        "\n",
        "  device_list = devices['device_id']\n",
        "  app_categories_df = app_labels_df.merge(label_categories_df, on='label_id', how='left')\n",
        "\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['event_id', 'device_id']]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  app_events = app_events.groupby('device_id').app_id.apply(lambda x: ' '.join([str(app_id) for app_id in x])).to_frame()\n",
        "  \n",
        "  app_events = devices.merge(app_events, on='device_id', how='left')\n",
        "  app_events['app_id'] = app_events['app_id'].fillna('na')\n",
        "\n",
        "  if training:\n",
        "    vocabulory = list(app_categories_df['app_id'].astype(str).unique()) + ['na']\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulory)\n",
        "    apps_bow_vector = vectorizer.fit_transform(app_events['app_id'])\n",
        "\n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return apps_bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    categories_bow_vector = vectorizer.transform(app_events['app_id'])\n",
        "    return categories_bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4AMIuGLKaTK"
      },
      "source": [
        "### 3. phone brand response encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4Bkb4UoKecs"
      },
      "outputs": [],
      "source": [
        "def get_phone_brand_features_response_enc(devices, phone_brand_df, y_train, label=None, training=False):\n",
        "  import pickle\n",
        "  import json\n",
        "\n",
        "  # if we want features for gender prediction \n",
        "  # then labels should be only 0, 1\n",
        "  # and otherwise, it would be 12 labels 0 to 11\n",
        "  if 'gender' in label:\n",
        "    labels = np.array([0, 1])\n",
        "  else:\n",
        "    labels = np.array(list(range(0,12)))\n",
        "\n",
        "  filename = f'{drive_path}/response_encoding_phone_brand{label}.pkl'\n",
        "\n",
        "  phone_brand_df = phone_brand_df.groupby('device_id').agg(\n",
        "      phone_brand = ('phone_brand', lambda x: ' '.join(np.unique(x)))\n",
        "  ).reset_index()\n",
        "\n",
        "  if training:\n",
        "    encoding_dict = {}\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model['label'] = y_train\n",
        "\n",
        "    for brand_name in brand_and_model['phone_brand'].unique():\n",
        "      if not brand_name:\n",
        "        continue\n",
        "\n",
        "      temp = brand_and_model[brand_and_model['phone_brand'] == brand_name]\n",
        "      totals = len(temp)\n",
        "      \n",
        "      encoding_list = []\n",
        "      for label in labels:\n",
        "        count = len(temp[temp['label'] == label])\n",
        "        encoding_list.append(count/totals)\n",
        "\n",
        "      encoding_dict.update({\n",
        "          brand_name: encoding_list\n",
        "      })\n",
        "      \n",
        "    brand_and_model[labels] = brand_and_model['phone_brand'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['phone_brand', 'label'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    # save to disk\n",
        "    json.dump(encoding_dict, open(filename, 'w'))\n",
        "\n",
        "    return brand_and_model\n",
        "  else:\n",
        "    encoding_dict = json.load(open(filename))\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model[labels] = brand_and_model['phone_brand'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['phone_brand'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    return brand_and_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68sLv36g_es"
      },
      "source": [
        "### 4. device model response encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_mwsXJqg_ti"
      },
      "outputs": [],
      "source": [
        "def get_device_model_features_response_enc(devices, phone_brand_df, y_train, label=None, training=False):\n",
        "  import pickle\n",
        "  import json\n",
        "\n",
        "  # if we want features for gender prediction \n",
        "  # then labels should be only 0, 1\n",
        "  # and otherwise, it would be 12 labels 0 to 11\n",
        "  if 'gender' in label:\n",
        "    labels = np.array([0, 1])\n",
        "  else:\n",
        "    labels = np.array(list(range(0,12)))\n",
        "\n",
        "  filename = f'{drive_path}/response_encoding_device_model_{label}.pkl'\n",
        "\n",
        "  phone_brand_df = phone_brand_df.groupby('device_id').agg(\n",
        "    device_model = ('device_model', lambda x: ' '.join(np.unique(x)))\n",
        "  ).reset_index()\n",
        "\n",
        "  if training:\n",
        "    encoding_dict = {}\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model['label'] = y_train\n",
        "\n",
        "    for device_model in brand_and_model['device_model'].unique():\n",
        "      if not device_model:\n",
        "        continue\n",
        "\n",
        "      temp = brand_and_model[brand_and_model['device_model'] == device_model]\n",
        "      totals = len(temp)\n",
        "      \n",
        "      encoding_list = []\n",
        "      for label in labels:\n",
        "        count = len(temp[temp['label'] == label])\n",
        "        encoding_list.append(count/totals)\n",
        "\n",
        "      encoding_dict.update({\n",
        "          device_model: encoding_list\n",
        "      })\n",
        "\n",
        "    brand_and_model[labels] = brand_and_model['device_model'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['device_model', 'label'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    # save to disk\n",
        "    json.dump(encoding_dict, open(filename, 'w'))\n",
        "    return brand_and_model\n",
        "  else:\n",
        "    encoding_dict = json.load(open(filename))\n",
        "\n",
        "    brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "    brand_and_model[labels] = brand_and_model['device_model'].apply(lambda x: pd.Series(encoding_dict.get(x)))\n",
        "    brand_and_model = brand_and_model.drop(['device_model'], axis=1)\n",
        "    brand_and_model = brand_and_model[labels].values\n",
        "    return brand_and_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFKnG_NFoazI"
      },
      "source": [
        "### 5. Bag of Phone brands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhQ9LEPQQwxi"
      },
      "outputs": [],
      "source": [
        "def get_phone_brand_features_bow(devices, phone_brand_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/bow_phone_brand.pkl'\n",
        "  vocabulary = phone_brand_df['phone_brand'].str.lower().unique().tolist()\n",
        "\n",
        "  brand_and_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "  brand_and_model = brand_and_model.groupby('device_id').phone_brand.apply(lambda x: ' '.join([s for s in x])).to_frame()\n",
        "  \n",
        "  if training:\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    bow_vector = vectorizer.fit_transform(brand_and_model['phone_brand'])\n",
        "    \n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    bow_vector = vectorizer.transform(brand_and_model['phone_brand'])\n",
        "    return bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8ynm7mwTQAG"
      },
      "source": [
        "### 6. Bag of device model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsCE8YKJTTu4"
      },
      "outputs": [],
      "source": [
        "def get_device_model_features_bow(devices, phone_brand_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/bow_device_model.pkl'\n",
        "  vocabulary = phone_brand_df['device_model'].str.lower().unique().tolist()\n",
        "\n",
        "  device_model = devices.merge(phone_brand_df, on='device_id', how='left')\n",
        "  device_model = device_model.groupby('device_id').device_model.apply(lambda x: ' '.join([s for s in x])).to_frame()\n",
        "\n",
        "  if training:\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    bow_vector = vectorizer.fit_transform(device_model['device_model'])\n",
        "    # saving the vectorizer to drive\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    bow_vector = vectorizer.transform(device_model['device_model'])\n",
        "    return bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtsymZKRUgaA"
      },
      "source": [
        "### 7. Total events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djl1U2q6Ugh_"
      },
      "outputs": [],
      "source": [
        "def get_total_events(devices, all_events_df, training=False):\n",
        "  events = all_events_df[all_events_df['device_id'].isin(devices['device_id'])]\n",
        "  events = devices.merge(events, on='device_id', how='left')\n",
        "  events = events.groupby('device_id').event_id.apply(lambda x: len(x)).to_frame()\n",
        "  return events['event_id'].values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9XfJm6nvKio"
      },
      "source": [
        "### 8. Total apps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alK3sMsFvKqx"
      },
      "outputs": [],
      "source": [
        "def total_apps_installed(devices, all_events_df, all_app_events_df, training=False):\n",
        "  device_list = devices['device_id']\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['event_id', 'device_id']]\n",
        "  app_events = all_app_events_df[all_app_events_df['event_id'].isin(events['event_id'])]\n",
        "  app_events = events.merge(app_events, on='event_id', how='right')\n",
        "  total_apps = app_events.groupby('device_id').agg({'app_id': 'nunique'}).reset_index()\n",
        "  total_apps = devices.merge(total_apps, on='device_id', how='left')\n",
        "  total_apps['app_id'] = total_apps.app_id.fillna(0)\n",
        "  total_apps = total_apps.drop('device_id', axis=1)\n",
        "  \n",
        "  return total_apps.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEYXOPIgwQPV"
      },
      "source": [
        "### 9. distance travelled"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distance_traveled_per_day(devices, all_events_df, training=False):\n",
        "    import pickle\n",
        "    filename = f'{drive_path}/distance_travelled_columns.sav'\n",
        "    def get_distance_travelled(x):\n",
        "        x = x.sort_values('datetime')\n",
        "        locations = x.to_dict('records')\n",
        "        total_d = 0\n",
        "        \n",
        "        for i, _ in enumerate(locations):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            \n",
        "            # ignoring the invalid lat and long co-ordinates\n",
        "            if locations[i-1].get('latitude') == 0.0 or locations[i].get('latitude') == 0.0 or locations[i-1].get('latitude') == 1.0 or locations[i].get('latitude') == 1.0:\n",
        "                continue\n",
        "\n",
        "            distance = great_circle(\n",
        "                (locations[i-1].get('latitude'), locations[i-1].get('longitude')), \n",
        "                (locations[i].get('latitude'), locations[i].get('longitude'))\n",
        "            ).km\n",
        "            total_d += distance\n",
        "        \n",
        "        return total_d\n",
        "    \n",
        "    device_list = devices['device_id']\n",
        "    device_events = all_events_df[all_events_df['device_id'].isin(device_list)].copy()\n",
        "    device_events['datetime'] = pd.to_datetime(device_events['timestamp'])\n",
        "    device_events['day'] = device_events['datetime'].dt.day\n",
        "\n",
        "    if training:\n",
        "      columns = list(device_events['day'].unique())\n",
        "      \n",
        "      # saving the vectorizer to drive\n",
        "      pickle.dump(columns, open(filename, 'wb'))\n",
        "    else:\n",
        "      columns = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "    def get_features(x):\n",
        "      features = {\n",
        "          int(i): 0\n",
        "          for i in columns\n",
        "      }\n",
        "      for row in x.iterrows():\n",
        "        key = int(row[1].day)\n",
        "        features[key] = row[1].distance\n",
        "\n",
        "      return pd.Series(list(features.values()))\n",
        "\n",
        "    # print('distance travelled 1')\n",
        "    device_events = device_events.groupby(['device_id', 'day'])[['datetime', 'latitude', 'longitude']].apply(\n",
        "      lambda x: get_distance_travelled(x)\n",
        "    ).to_frame().reset_index().rename(columns={0: 'distance'})\n",
        "    # print('distance travelled 2')\n",
        "    device_events = device_events.groupby('device_id').apply(get_features)\n",
        "    print('distance travelled ', device_events.shape)\n",
        "    return device_events.values"
      ],
      "metadata": {
        "id": "fovBZihgx0C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# devices = pd.read_csv('data/gender_age_train.csv')\n",
        "# distance_travelled = distance_traveled_per_day(devices, all_events_df, training=False)"
      ],
      "metadata": {
        "id": "Ocmyse_JzCqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vRQG-6zvAnI"
      },
      "source": [
        "### 10. Has events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B7q9Zs2vAwX"
      },
      "outputs": [],
      "source": [
        "def has_events(devices, all_events_df):\n",
        "  import math\n",
        "\n",
        "  device_has_events = all_events_df.groupby('device_id').agg({'event_id': 'nunique'}).reset_index()\n",
        "  device_events = devices.merge(device_has_events, on='device_id', how='left')\n",
        "  device_events['has_events'] = pd.notna(device_events['event_id'])\n",
        "  device_events['has_events'] = device_events['has_events'].replace({True: 1})\n",
        "  device_events['has_events'] = device_events['has_events'].replace({False: 0})\n",
        "  return device_events['has_events'].values    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. hourly events"
      ],
      "metadata": {
        "id": "KxyLkg9DV5_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hourly_events(devices, all_events_df, training=False):\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/hourly_events.pkl'\n",
        "\n",
        "  events = all_events_df[all_events_df['device_id'].isin(devices['device_id'])]\n",
        "  events['hour'] = pd.to_datetime(events['timestamp']).dt.hour\n",
        "  events = events.groupby(['device_id']).hour.apply(lambda x: ' '.join([str(s) for s in x])).to_frame().reset_index()\n",
        "  events = devices.merge(events, on='device_id', how='left')\n",
        "  events.hour = events.hour.fillna('na')\n",
        "\n",
        "  if training:\n",
        "    vocabulary = [str(h) for h in range(0, 24)] + ['na']\n",
        "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    bow_vector = vectorizer.fit_transform(events['hour'])\n",
        "    pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "    return bow_vector\n",
        "  else:\n",
        "    vectorizer = pickle.load(open(filename, 'rb'))\n",
        "    bow_vector = vectorizer.transform(events['hour'])\n",
        "    return bow_vector"
      ],
      "metadata": {
        "id": "m1Ah0Mu3V9Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXjSvXwm7EF3"
      },
      "source": [
        "### 11. median latitude and longitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ0Q4tWA7EZP"
      },
      "outputs": [],
      "source": [
        "def get_median_lat_long(devices, all_events_df):\n",
        "  device_list = devices['device_id']\n",
        "  events = all_events_df[all_events_df['device_id'].isin(device_list)][['device_id', 'latitude', 'longitude']]\n",
        "  \n",
        "  events.longitude=events.longitude.apply(lambda x:np.NaN if x==0 else x) \n",
        "  events.latitude=events.latitude.apply(lambda x:np.NaN if x==0 else x)\n",
        "\n",
        "  lat_long = events.groupby('device_id').agg({\n",
        "      'latitude': 'median',\n",
        "      'longitude': 'median',\n",
        "  }).reset_index()\n",
        "  lat_long = devices.merge(lat_long, on='device_id', how='left')\n",
        "  lat_long[['latitude', 'longitude']] = lat_long[['latitude', 'longitude']].fillna(0)\n",
        "  \n",
        "  return lat_long[['latitude', 'longitude']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrbvKFoPBrrw"
      },
      "source": [
        "### Extracting features in training data and save intermediate models to use on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwBpoO2FIs5n"
      },
      "outputs": [],
      "source": [
        "# all_events_df = pd.read_csv('data/events.csv')\n",
        "# all_app_events_df = pd.read_csv('data/app_events.csv')\n",
        "# app_labels_df = pd.read_csv('data/app_labels.csv')\n",
        "# label_categories_df = pd.read_csv('data/label_categories.csv')\n",
        "# phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY6idenCyfux"
      },
      "source": [
        "## Get features for training data.\n",
        "<p>We will also save the counter vectorizers used for multiple features, which we can use while preparing features for test data</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPdGwlxZW8g"
      },
      "outputs": [],
      "source": [
        "def select_k_best(features, y_train, label, n=1000, training=False):\n",
        "  from sklearn.feature_selection import SelectKBest, chi2\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/select_k_best_{label}.pkl'\n",
        "\n",
        "  if training:\n",
        "    kbest = SelectKBest(chi2, k=n)\n",
        "    best_features = kbest.fit_transform(features, y_train)\n",
        "    pickle.dump(kbest, open(filename, 'wb'))\n",
        "    return best_features\n",
        "  else:\n",
        "    kbest = pickle.load(open(filename, 'rb'))\n",
        "    best_features = kbest.transform(features)\n",
        "    return best_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wnVYjEVb7-j"
      },
      "outputs": [],
      "source": [
        "def remove_features_with_no_variance(features, training=False, label='with_events'):\n",
        "  from sklearn.feature_selection import VarianceThreshold\n",
        "  import pickle\n",
        "  filename = f'{drive_path}/no_variance_model_{label}.pkl'\n",
        "  \n",
        "  if training:\n",
        "    selector = VarianceThreshold(threshold=0.0)\n",
        "    best_features = selector.fit_transform(features)\n",
        "    pickle.dump(selector, open(filename, 'wb'))\n",
        "    return best_features\n",
        "  else:\n",
        "    selector = pickle.load(open(filename, 'rb'))\n",
        "    best_features = selector.transform(features)\n",
        "    return best_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPh1Pnimyf5o"
      },
      "outputs": [],
      "source": [
        "def get_features_with_events(train_data_devices, y_train, training=False, label='gender'):\n",
        "  all_events_df = pd.read_csv('data/events.csv')\n",
        "  all_app_events_df = pd.read_csv('data/app_events.csv')\n",
        "  app_labels_df = pd.read_csv('data/app_labels.csv')\n",
        "  label_categories_df = pd.read_csv('data/label_categories.csv')\n",
        "  phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')\n",
        "\n",
        "  categories_bow = get_bag_of_categories(\n",
        "    train_data_devices, all_events_df, all_app_events_df, \n",
        "    app_labels_df, label_categories_df, training=training\n",
        "  )\n",
        "  apps_bow = get_bag_of_apps(train_data_devices, app_labels_df, all_events_df, all_app_events_df, label_categories_df, training=False)\n",
        "  phone_brand_resp_enc = get_phone_brand_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_with_events', training=training)\n",
        "  device_model_resp_enc = get_device_model_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_with_events', training=training)\n",
        "\n",
        "  bow_phone_brand = get_phone_brand_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  bow_device_model = get_device_model_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  total_events = get_total_events(train_data_devices, all_events_df, training=training)\n",
        "  total_apps = total_apps_installed(train_data_devices, all_events_df, all_app_events_df, training=training)\n",
        "  distance_travelled = distance_traveled_per_day(train_data_devices, all_events_df, training=training)\n",
        "  median_lat_long = get_median_lat_long(train_data_devices, all_events_df)\n",
        "  hourly_events = get_hourly_events(train_data_devices, all_events_df, training=training)\n",
        "\n",
        "  apps_bow_best = select_k_best(apps_bow, y_train, label=label, n=2000, training=training)\n",
        "  # total_events\n",
        "  X = hstack([categories_bow, apps_bow_best, \n",
        "            bow_phone_brand, bow_device_model, \n",
        "            total_apps, distance_travelled, median_lat_long, hourly_events])\n",
        "  \n",
        "  print('data shape before removing no variance data: ', X.shape)\n",
        "  print('training: ', training)\n",
        "  X = remove_features_with_no_variance(X, training=training, label=f'{label}_with_events')\n",
        "  print('data shape', X.shape)\n",
        "\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMBPUAsnuSw9"
      },
      "outputs": [],
      "source": [
        "def get_features_with_no_events(train_data_devices, y_train, training=False, label='gender'):\n",
        "  phone_brand_df = pd.read_csv('data/phone_brand_device_model.csv')\n",
        "  bow_phone_brand = get_phone_brand_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  bow_device_model = get_device_model_features_bow(train_data_devices, phone_brand_df, training=training)\n",
        "  phone_brand_resp_enc = get_phone_brand_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_no_events', training=training)\n",
        "  device_model_resp_enc = get_device_model_features_response_enc(\n",
        "      train_data_devices, phone_brand_df, y_train, label=f'{label}_no_events', training=training)\n",
        "  X = hstack([bow_phone_brand, bow_device_model, phone_brand_resp_enc,device_model_resp_enc])\n",
        "  X = remove_features_with_no_variance(X, training=training, label=f'{label}_no_events')\n",
        "  print('data shape', X.shape)\n",
        "  \n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mHmCXtFLzzp"
      },
      "outputs": [],
      "source": [
        "def get_labels(y, training=False, label='group'):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  import pickle\n",
        "\n",
        "  filename = f'{drive_path}/label_encoder_{label}.pkl'\n",
        "\n",
        "  if training:\n",
        "    enc = LabelEncoder()\n",
        "    y =  enc.fit_transform(y)\n",
        "    pickle.dump(enc, open(filename, 'wb'))\n",
        "    return y\n",
        "  else:\n",
        "    enc = pickle.load(open(filename, 'rb'))\n",
        "    y =  enc.transform(y)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY-sg3RIyh34"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7ePo4QPyiBR"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(test_y, predict_y, class_labels):\n",
        "    C = confusion_matrix(test_y, predict_y)\n",
        "    print(\"Number of misclassified points \",(len(test_y)-np.trace(C))/len(test_y)*100)\n",
        "    \n",
        "    A =(((C.T)/(C.sum(axis=1))).T)\n",
        "    \n",
        "    B =(C/C.sum(axis=0))\n",
        "    print(1)\n",
        "    labels = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "    cmap=sns.light_palette(\"green\")\n",
        "\n",
        "    # representing A in heatmap format\n",
        "    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(2)\n",
        "    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(3)\n",
        "    # representing B in heatmap format\n",
        "    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2e3b-b8AMbs"
      },
      "source": [
        "## Predicting gender for devices with events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGcYSRLdAMpN"
      },
      "outputs": [],
      "source": [
        "def train_gender_predictor_with_events(devices_events_gen, y_gen_label, training=False):\n",
        "  X_train = get_features_with_events(devices_events_gen, y_gen_label, training=training, label='gender')\n",
        "  if training:\n",
        "    print('train dataset\\'s shape')\n",
        "    print(X_train.shape)\n",
        "    print(y_gen_label.shape)\n",
        "\n",
        "    gpu_dict = {\n",
        "      'objective': 'binary:logistic',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"logloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_train, y_gen_label)\n",
        "\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_train, y_gen_label)\n",
        "          \n",
        "    predict_y_train_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_train = cali_cfl.predict(X_train)\n",
        "      \n",
        "    # saving the model to disk for later usage\n",
        "    import pickle\n",
        "    file_name = f\"{drive_path}/best_model_gender_with_events.pkl\"\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y_train, predict_y_train_proba\n",
        "  else:\n",
        "    print('testing')\n",
        "    import pickle\n",
        "    file_name = f\"{drive_path}/best_model_gender_with_events.pkl\"\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_test_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_test = cali_cfl.predict(X_train)\n",
        "    return predict_y_test, predict_y_test_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZY9XPcxapQX"
      },
      "source": [
        "## Predicting gender based on devices with no events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJE_8-9Hapf9"
      },
      "outputs": [],
      "source": [
        "def train_gender_predictor_with_no_events(devices_no_events_gen, y_gen_label, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_gender_with_no_events.pkl\"\n",
        "  X_train = get_features_with_no_events(devices_no_events_gen, y_gen_label, training=training, label='gender')\n",
        "  if training:\n",
        "    print('train dataset\\'s shape')\n",
        "    print(X_train.shape)\n",
        "    print(y_gen_label.shape)\n",
        "\n",
        "    gpu_dict = {\n",
        "      'objective': 'binary:logistic',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"logloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_train, y_gen_label)\n",
        "\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_train, y_gen_label)\n",
        "          \n",
        "    predict_y_train_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_train = cali_cfl.predict(X_train)\n",
        "      \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y_train, predict_y_train_proba\n",
        "  else:\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_test_proba = cali_cfl.predict_proba(X_train)\n",
        "    predict_y_test = cali_cfl.predict(X_train)\n",
        "    return predict_y_test, predict_y_test_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV5rBrITIZiQ"
      },
      "source": [
        "## Predicting user groups on devices with events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV-QuntbIZv3"
      },
      "outputs": [],
      "source": [
        "def train_user_group_with_events(devices_events, gender_pred, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_group_with_events.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_events(devices_events, y_tr_events, training=training, label='group')\n",
        "  X_events = hstack([X_events, gender_pred])\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_events, y_tr_events)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    print('testing part')\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    return predict_y, predict_y_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkjEMigHcI9W"
      },
      "source": [
        "## Predicting user group on devices with no events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSBCw1XFcJEP"
      },
      "outputs": [],
      "source": [
        "def train_user_group_with_no_events(devices_events, gender_pred, y_tr_events, training=False):\n",
        "  import pickle\n",
        "  file_name = f\"{drive_path}/best_model_group_with_no_events.pkl\"\n",
        "    \n",
        "  X_events = get_features_with_no_events(devices_events, y_tr_events, training=training, label='group')\n",
        "  X_events = hstack([X_events, gender_pred])\n",
        "  \n",
        "  if training:\n",
        "    gpu_dict = {\n",
        "      'objective': 'multi:softproba',\n",
        "      'tree_method': 'gpu_hist',\n",
        "      \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "\n",
        "    param_grid = {'max_depth': [2, 3, 4], 'n_estimators': [200, 300, 400, 500]}\n",
        "    xgb = XGBClassifier(**gpu_dict, random_state=42)\n",
        "\n",
        "    gs = GridSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_grid=param_grid, \n",
        "            cv=4, \n",
        "            n_jobs=-1, \n",
        "            scoring='neg_log_loss',\n",
        "            verbose=10\n",
        "        )\n",
        "    gs.fit(X_events, y_tr_events)\n",
        "    print('Best params ', gs.best_params_)\n",
        "    x_cfl = gs.best_estimator_\n",
        "        \n",
        "    cali_cfl = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    cali_cfl.fit(X_events, y_tr_events)\n",
        "          \n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    \n",
        "    # saving the model to disk for later usage\n",
        "    pickle.dump(cali_cfl, open(file_name, \"wb\"))\n",
        "    return predict_y, predict_y_proba\n",
        "  else:\n",
        "    # loading the model\n",
        "    cali_cfl = pickle.load(open(file_name, \"rb\"))\n",
        "    predict_y_proba = cali_cfl.predict_proba(X_events)\n",
        "    predict_y = cali_cfl.predict(X_events)\n",
        "    return predict_y, predict_y_proba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8DKLagS-6HN"
      },
      "source": [
        "## Method to train and cross validate both models on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUvQlhoj-6OU"
      },
      "outputs": [],
      "source": [
        "def train_models(all_events_df):\n",
        "  devices = pd.read_csv('data/gender_age_train.csv')\n",
        "  y = devices['group'].values\n",
        "  y_gender = devices['gender'].values\n",
        "  is_events = has_events(devices, all_events_df)\n",
        "\n",
        "  # predicting gender with devices on events data\n",
        "  devices_with_events = devices[is_events == True][['device_id']]\n",
        "  y_gender_with_events = y_gender[is_events == True]\n",
        "\n",
        "  tr_devices_events_gen, te_devices_events_gen, y_tr_events_gen, y_te_events_gen = train_test_split(\n",
        "    devices_with_events, y_gender_with_events, test_size=0.1, stratify=y_gender_with_events\n",
        "  )\n",
        "  y_tr_gen_label = get_labels(y_tr_events_gen, training=True, label='gender')\n",
        "  y_te_gen_label = get_labels(y_te_events_gen, training=False, label='gender')\n",
        "\n",
        "  y_train_gen_pred, y_train_gen_pred_proba = train_gender_predictor_with_events(tr_devices_events_gen, y_tr_gen_label, training=True)\n",
        "  \n",
        "  y_test_gen_pred, y_test_gen_pred_proba = train_gender_predictor_with_events(te_devices_events_gen, y_te_gen_label, training=False)\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_gen_label, y_train_gen_pred_proba))\n",
        "  print (\"The train log loss is:\", log_loss(y_te_gen_label, y_test_gen_pred_proba))\n",
        "\n",
        "  # predicting group on devices with events\n",
        "  y_with_events = y[is_events == True]\n",
        "  tr_devices_events, te_devices_events, y_tr_events, y_te_events = train_test_split(\n",
        "    devices_with_events, y_with_events, test_size=0.1, stratify=y_with_events\n",
        "  )\n",
        "  y_tr_label = get_labels(y_tr_events, training=True, label='group')\n",
        "  y_te_label = get_labels(y_te_events, training=False, label='group')\n",
        "\n",
        "  y_train_pred, y_train_pred_proba = train_user_group_with_events(tr_devices_events, y_train_gen_pred_proba, y_tr_label, training=True)\n",
        "  y_test_pred, y_test_pred_proba = train_user_group_with_events(te_devices_events, y_test_gen_pred_proba, y_te_label, training=False)\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_label, y_train_pred_proba))\n",
        "  print (\"The train log loss is:\", log_loss(y_te_label, y_test_pred_proba))\n",
        "\n",
        "  # predicting gender on devices without events\n",
        "  devices_no_events = devices[is_events == False][['device_id']]\n",
        "  y_gender_no_events = y_gender[is_events == False]\n",
        "\n",
        "  tr_devices_no_events_gen, te_devices_no_events_gen, y_tr_no_events_gen, y_te_no_events_gen = train_test_split(\n",
        "    devices_no_events, y_gender_no_events, test_size=0.1, stratify=y_gender_no_events\n",
        "  )\n",
        "  y_tr_no_events_gen_label = get_labels(y_tr_no_events_gen, training=True, label='gender')\n",
        "  y_te_no_events_gen_label = get_labels(y_te_no_events_gen, training=False, label='gender')\n",
        "\n",
        "  y_train_gen_pred_no_events, y_train_gen_pred_proba_no_events = train_gender_predictor_with_no_events(\n",
        "      tr_devices_no_events_gen, y_tr_no_events_gen_label, training=True)\n",
        "  y_test_gen_pred_no_events, y_test_gen_pred_proba_no_events = train_gender_predictor_with_no_events(\n",
        "      te_devices_no_events_gen, y_te_no_events_gen_label, training=False\n",
        "    )\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_no_events_gen_label, y_train_gen_pred_proba_no_events))\n",
        "  print (\"The test log loss is:\", log_loss(y_te_no_events_gen_label, y_test_gen_pred_proba_no_events))\n",
        "\n",
        "\n",
        "  # predicting gender on devices without events\n",
        "  y_no_events = y[is_events == False]\n",
        "\n",
        "  tr_devices_no_events, te_devices_no_events, y_tr_no_events, y_te_no_events = train_test_split(\n",
        "    devices_no_events, y_no_events, test_size=0.1, stratify=y_no_events\n",
        "  )\n",
        "  y_tr_no_events_label = get_labels(y_tr_no_events, training=True, label='group')\n",
        "  y_te_no_events_label = get_labels(y_te_no_events, training=False, label='group')\n",
        "\n",
        "  y_train_pred_no_events, y_train_pred_no_events_proba = train_user_group_with_no_events(\n",
        "      tr_devices_no_events, y_train_gen_pred_proba_no_events, y_tr_no_events_label, training=True)\n",
        "  y_test_pred_no_events, y_test_pred_no_events_proba = train_user_group_with_no_events(\n",
        "      te_devices_no_events, y_test_gen_pred_proba_no_events, y_te_no_events_label, training=False)\n",
        "  print (\"The train log loss is:\", log_loss(y_tr_no_events_label, y_train_pred_no_events_proba))\n",
        "  print (\"The train log loss is:\", log_loss(y_te_no_events_label, y_test_pred_no_events_proba))\n",
        "\n",
        "\n",
        "  y_true = np.concatenate([y_te_label, y_te_no_events_label])\n",
        "  y_pred = np.concatenate([y_test_pred_proba, y_test_pred_no_events_proba])\n",
        "\n",
        "  print (\"The overall test log loss is:\", log_loss(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  all_events_df = pd.read_csv('data/events.csv')\n",
        "  train_models(all_events_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fbl_1rgisdb",
        "outputId": "ddd9870a-43b7-4ede-b528-9424576f6b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance travelled  (20978, 9)\n",
            "data shape before removing no variance data:  (20978, 4591)\n",
            "training:  True\n",
            "data shape (20978, 2843)\n",
            "train dataset's shape\n",
            "(20978, 2843)\n",
            "(20978,)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 400}\n",
            "distance travelled  (2331, 9)\n",
            "data shape before removing no variance data:  (2331, 4591)\n",
            "training:  False\n",
            "data shape (2331, 2843)\n",
            "testing\n",
            "The train log loss is: 0.4461570421436561\n",
            "The train log loss is: 0.4991837028592509\n",
            "distance travelled  (20978, 9)\n",
            "data shape before removing no variance data:  (20978, 4591)\n",
            "training:  True\n",
            "data shape (20978, 2842)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 300}\n",
            "distance travelled  (2331, 9)\n",
            "data shape before removing no variance data:  (2331, 4591)\n",
            "training:  False\n",
            "data shape (2331, 2842)\n",
            "testing part\n",
            "The train log loss is: 1.759920530655525\n",
            "The train log loss is: 2.1593722289610655\n",
            "data shape (46202, 1043)\n",
            "train dataset's shape\n",
            "(46202, 1043)\n",
            "(46202,)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 200}\n",
            "data shape (5134, 1043)\n",
            "The train log loss is: 0.6197904356297556\n",
            "The test log loss is: 0.6507084243612778\n",
            "data shape (46202, 1068)\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best params  {'max_depth': 2, 'n_estimators': 200}\n",
            "data shape (5134, 1068)\n",
            "The train log loss is: 2.232829156362378\n",
            "The train log loss is: 2.415260231061176\n",
            "The overall test log loss is: 2.3353573599432447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "ZKCRbpeSvOL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   With same number of features and using the single model to predict user group, was giving very poor results.\n",
        "*   So, we, train four different models as described below along with their corresponding log loss\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8lZ5g4PvPzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "data = [\n",
        "        ['Yes', \"Gender\", 0.49164734136922056],\n",
        "        [\"Yes\", \"User group (used predicted gender from above model as new feature)\", 2.133802776693025],\n",
        "        [\"No\", \"Gender\", 0.6511259420027772],\n",
        "        [\"No\", \"User group (used predicted gender from above model as new feature)\", 2.420144409974926]\n",
        "      ]\n",
        "print(tabulate(data, headers=[\"Is events data available?\", \"What we are predicting?\", \"Test log loss\"]))"
      ],
      "metadata": {
        "id": "oTCj9zhjvyw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_pBrxgu6F8WD",
        "9xs6fhqVGBut",
        "f4AMIuGLKaTK",
        "E68sLv36g_es",
        "GFKnG_NFoazI",
        "X8ynm7mwTQAG",
        "E9XfJm6nvKio",
        "9vRQG-6zvAnI",
        "QY-sg3RIyh34",
        "yZY9XPcxapQX",
        "GV5rBrITIZiQ",
        "HkjEMigHcI9W"
      ],
      "name": "model_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}